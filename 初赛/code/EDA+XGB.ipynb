{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型一：XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "operation_TRAIN = pd.read_csv(\"../data/operation_TRAIN.csv\")\n",
    "transaction_TRAIN = pd.read_csv(\"../data/transaction_TRAIN.csv\")\n",
    "tag_TRAIN = pd.read_csv(\"../data/tag_TRAIN.csv\")                                                          \n",
    "operation_TRAIN = operation_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "transaction_TRAIN = transaction_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "train_X = pd.merge(operation_TRAIN, transaction_TRAIN, on=\"UID\", how=\"outer\")\n",
    "train = pd.merge(train_X, tag_TRAIN, on=\"UID\", how=\"left\")\n",
    "train = train.sort_values(\"UID\")\n",
    "\n",
    "pos_train = train[train[\"Tag\"] == 1]\n",
    "\n",
    "\n",
    "operation_train_new = pd.read_csv(\"../data/operation_train_new.csv\")\n",
    "transaction_train_new = pd.read_csv(\"../data/transaction_train_new.csv\")\n",
    "tag_train_new = pd.read_csv(\"../data/tag_train_new.csv\")                                                          \n",
    "operation_train_new = operation_train_new.drop_duplicates(\"UID\", keep=\"last\")\n",
    "transaction_train_new = transaction_train_new.drop_duplicates(\"UID\", keep=\"last\")\n",
    "train_X_new = pd.merge(operation_train_new, transaction_train_new, on=\"UID\", how=\"outer\")\n",
    "train_new = pd.merge(train_X_new, tag_train_new, on=\"UID\", how=\"left\")\n",
    "train_new = train_new.sort_values(\"UID\")\n",
    "\n",
    "train_all = pd.concat([pos_train, train_new], axis=0)\n",
    "# operation_round1 = pd.read_csv(\"../data/operation_round1.csv\")\n",
    "# transaction_round1 = pd.read_csv(\"../data/transaction_round1.csv\")                                                                                                                      \n",
    "# operation_round1 = operation_round1.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "# transaction_round1 = transaction_round1.drop_duplicates([\"UID\"], keep=\"last\")                                                              \n",
    "# test_C = pd.merge(operation_round1, transaction_round1, on=\"UID\", how=\"outer\")\n",
    "# test_C = test_C.sort_values(\"UID\")\n",
    "\n",
    "operation_round1_new = pd.read_csv(\"../data/operation_round1_new.csv\")\n",
    "transaction_round1_new = pd.read_csv(\"../data/transaction_round1_new.csv\")                                                                                                                      \n",
    "operation_round1_new = operation_round1_new.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "transaction_round1_new = transaction_round1_new.drop_duplicates([\"UID\"], keep=\"last\")                                                              \n",
    "test_C_new = pd.merge(operation_round1_new, transaction_round1_new, on=\"UID\", how=\"outer\")\n",
    "test_C_new = test_C_new.sort_values(\"UID\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del operation_TRAIN\n",
    "del transaction_TRAIN\n",
    "del tag_TRAIN\n",
    "del operation_train_new\n",
    "del transaction_train_new\n",
    "del tag_train_new\n",
    "del operation_round1_new\n",
    "del transaction_round1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_C_new[\"Tag\"] = -1\n",
    "data = pd.concat([train_all, test_C_new])\n",
    "data[\"time_x\"] = data[\"time_x\"].fillna(value=\"00:00:00\")\n",
    "data[\"time_y\"] = data[\"time_y\"].fillna(value=\"00:00:00\")\n",
    "data[\"day_x\"] = data[\"day_x\"].fillna(value=0)\n",
    "data[\"day_y\"] = data[\"day_y\"].fillna(value=30)\n",
    "data = data.fillna(value=-99999)\n",
    "\n",
    "data[\"hour_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[0])\n",
    "data[\"minutes_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[1])\n",
    "data[\"hour_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[0])\n",
    "data[\"minutes_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[1])\n",
    "\n",
    "data[\"day_y\"] = data[\"day_y\"] + 30\n",
    "data[\"day_sub\"] = data[\"day_y\"] - data[\"day_x\"]\n",
    "data[\"hour_sub\"] = (data[\"hour_y\"].astype(int) +24) - data[\"hour_x\"].astype(int)\n",
    "data[\"minutes_sub\"] = (data[\"minutes_y\"].astype(int) +60) - data[\"minutes_x\"].astype(int)\n",
    "\n",
    "feature = data.columns.drop([\"UID\", \"Tag\"])\n",
    "col_feature = [\"day_x\", \"hour_sub\", \"minutes_sub\", \"bal\", \"trans_amt\"]\n",
    "dis_feature = feature.drop(col_feature)\n",
    "\n",
    "for col in dis_feature:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    \n",
    "# data[\"time_sub\"] = data[\"time_y\"] - data[\"time_x\"]\n",
    "# data[\"geo_code_sub\"] = data[\"geo_code_y\"] - data[\"geo_code_x\"]\n",
    "\n",
    "# data[\"device21_x_sub\"] = data[\"device2_x\"] - data[\"device1_x\"]\n",
    "# data[\"device21_y_sub\"] = data[\"device2_y\"] - data[\"device1_y\"]\n",
    "# data[\"device1_sub\"] = data[\"device1_y\"] - data[\"device1_x\"]\n",
    "# data[\"device2_sub\"] = data[\"device2_y\"] - data[\"device2_x\"]\n",
    "\n",
    "# data[\"device_code21_x_sub\"] = data[\"device_code2_x\"] - data[\"device_code1_x\"]\n",
    "# data[\"device_code32_x_sub\"] = data[\"device_code3_x\"] - data[\"device_code2_x\"]\n",
    "# data[\"device_code31_x_sub\"] = data[\"device_code3_x\"] - data[\"device_code1_x\"]\n",
    "# data[\"device_code21_y_sub\"] = data[\"device_code2_y\"] - data[\"device_code1_y\"]\n",
    "# data[\"device_code32_y_sub\"] = data[\"device_code3_y\"] - data[\"device_code2_y\"]\n",
    "# data[\"device_code31_y_sub\"] = data[\"device_code3_y\"] - data[\"device_code1_y\"]\n",
    "# data[\"device_code1_sub\"] = data[\"device_code1_y\"] - data[\"device_code1_x\"]\n",
    "# data[\"device_code2_sub\"] = data[\"device_code2_y\"] - data[\"device_code2_x\"]\n",
    "# data[\"device_code3_sub\"] = data[\"device_code3_y\"] - data[\"device_code3_x\"]\n",
    "\n",
    "# data[\"mac1_sub\"] = data[\"mac1_y\"] - data[\"mac1_x\"]\n",
    "# data[\"mac_sub\"] = data[\"mac2\"] - data[\"mac1_x\"]\n",
    "# data[\"ip1_sub\"] = data[\"ip1_y\"] - data[\"ip1_x\"]\n",
    "# data[\"ip21_sub\"] = data[\"ip2_sub\"] - data[\"ip1_sub\"]\n",
    "# data[\"ip1_sub_sub\"] = data[\"ip1_sub_y\"] - data[\"ip1_sub_x\"]\n",
    "# data[\"acc_id21_sub\"] = data[\"acc_id2\"] - data[\"acc_id1\"]\n",
    "# data[\"acc_id32_sub\"] = data[\"acc_id3\"] - data[\"acc_id2\"]\n",
    "# data[\"acc_id31_sub\"] = data[\"acc_id3\"] - data[\"acc_id1\"]\n",
    "    \n",
    "\n",
    "\n",
    "# for col in [\"mode\", \"os\", \"wifi\"]:\n",
    "#     print(\"this is feature:\", col)\n",
    "#     onehot_feats = pd.get_dummies(data[col], prefix=col)\n",
    "#     data.drop([col], axis=1, inplace=True)\n",
    "#     data = pd.concat([data, onehot_feats], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "train = data[data[\"Tag\"] != -1]\n",
    "train_Y = train[\"Tag\"]\n",
    "train_X = train.drop([\"UID\", \"Tag\"], axis=1)\n",
    "test = data[data[\"Tag\"] == -1]\n",
    "test_UID = test[\"UID\"]\n",
    "test_X = test.drop([\"UID\", \"Tag\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"booster\": \"gbtree\",\n",
    "          \"objective\" : \"binary:logistic\",\n",
    "          \"eval_metric\" : \"auc\",\n",
    "          \"verbose\" : 0, \n",
    "          \"eta\" : 0.01, \n",
    "          \"max_delta_step\" : 5, \n",
    "          \"max_depth\" : 20, \n",
    "          \"alpha\" : 10, \n",
    "          \"lambda\" : 10, \n",
    "          \"gamma\" : 2, \n",
    "          \"subsample\" : 0.8, \n",
    "          \"colsample_bytree\" : 0.8, \n",
    "          \"min_child_weight\" : 5, \n",
    "          \"scale_pos_weight\" : 0.8,\n",
    "          \"n_jobs\" : -1}\n",
    "\n",
    "# x_train, x_val, y_train, y_true = train_test_split(train_X, train_Y, test_size=0.2, random_state=2018)\n",
    "\n",
    "# matrix_train = xgb.DMatrix(x_train, y_train)\n",
    "# matrix_test = xgb.DMatrix(x_val)\n",
    "# a_model = xgb.train(params, matrix_train, num_boost_round=100000)\n",
    "# y_predict = a_model.predict(matrix_test)\n",
    "\n",
    "\n",
    "# d = pd.DataFrame()\n",
    "# d['prob'] = list(y_predict)\n",
    "# d['y'] = list(y_true)\n",
    "# d = d.sort_values(['prob'], ascending=[0])\n",
    "# y = d.y\n",
    "# PosAll = pd.Series(y).value_counts()[1]\n",
    "# NegAll = pd.Series(y).value_counts()[0]\n",
    "# pCumsum = d['y'].cumsum()\n",
    "# nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "# pCumsumPer = pCumsum / PosAll\n",
    "# nCumsumPer = nCumsum / NegAll\n",
    "# TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "# TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "# TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "\n",
    "# result = 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "# print(\"*******************************************************************************************************\")\n",
    "# print(\"result:\", result)\n",
    "# print(\"*******************************************************************************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_matrix = xgb.DMatrix(train_X, train_Y)\n",
    "test_matrix = xgb.DMatrix(test_X)\n",
    "\n",
    "model = xgb.train(params, train_matrix, num_boost_round=50000)\n",
    "predict = model.predict(test_matrix)\n",
    "\n",
    "result = pd.DataFrame({\"UID\": test_UID, \"Tag\": predict})\n",
    "result.to_csv(\"../submission/\"+\"result_time\"+str(int(time.time()))+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型二：LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "operation_train_new = pd.read_csv(\"../data/operation_train_new.csv\")\n",
    "transaction_train_new = pd.read_csv(\"../data/transaction_train_new.csv\")\n",
    "tag_train_new = pd.read_csv(\"../data/tag_train_new.csv\")                                                          \n",
    "operation_train_new = operation_train_new.drop_duplicates(\"UID\", keep=\"last\")\n",
    "transaction_train_new = transaction_train_new.drop_duplicates(\"UID\", keep=\"last\")\n",
    "train_X_new = pd.merge(operation_train_new, transaction_train_new, on=\"UID\", how=\"outer\")\n",
    "train_new = pd.merge(train_X_new, tag_train_new, on=\"UID\", how=\"left\")\n",
    "train_new = train_new.sort_values(\"UID\")\n",
    "\n",
    "operation_round1_new = pd.read_csv(\"../data/operation_round1_new.csv\")\n",
    "transaction_round1_new = pd.read_csv(\"../data/transaction_round1_new.csv\")                                                                                                                      \n",
    "operation_round1_new = operation_round1_new.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "transaction_round1_new = transaction_round1_new.drop_duplicates([\"UID\"], keep=\"last\")                                                              \n",
    "test_C = pd.merge(operation_round1_new, transaction_round1_new, on=\"UID\", how=\"outer\")\n",
    "test_C = test_C.sort_values(\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del operation_train_new\n",
    "del transaction_train_new\n",
    "del tag_train_new\n",
    "del operation_round1_new\n",
    "del transaction_round1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_C[\"Tag\"] = -1\n",
    "data = pd.concat([train_new, test_C])\n",
    "data = data.drop([\"time_x\", \"time_y\"], axis=1)\n",
    "feature = data.columns.drop([\"UID\", \"Tag\"])\n",
    "col_feature = [\"day_x\", \"day_y\", \"bal\", \"trans_amt\"]\n",
    "dis_feature = feature.drop(col_feature)\n",
    "\n",
    "for col in dis_feature:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    \n",
    "train = data[data[\"Tag\"] != -1]\n",
    "train_Y = train[\"Tag\"]\n",
    "train_X = train.drop([\"UID\", \"Tag\"], axis=1)\n",
    "test = data[data[\"Tag\"] == -1]\n",
    "test_UID = test[\"UID\"]\n",
    "test_X = test.drop([\"UID\", \"Tag\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tpr_weight_funtion(y_true, y_predict):\n",
    "#     d = pd.DataFrame()\n",
    "#     d['prob'] = list(y_predict)\n",
    "#     d['y'] = list(y_true)\n",
    "#     d = d.sort_values(['prob'], ascending=[0])\n",
    "#     y = d.y\n",
    "#     PosAll = pd.Series(y).value_counts()[1]\n",
    "#     NegAll = pd.Series(y).value_counts()[0]\n",
    "#     pCumsum = d['y'].cumsum()\n",
    "#     nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "#     pCumsumPer = pCumsum / PosAll\n",
    "#     nCumsumPer = nCumsum / NegAll\n",
    "#     TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "#     TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "#     TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "     \n",
    "#     return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "\n",
    "res = []\n",
    "params = {'boosting_type': 'gbdt',\n",
    "         'objective': 'binary',\n",
    "         'metric': 'auc',\n",
    "         'min_child_weight': 1.5,\n",
    "         'num_leaves': 2 ** 5,\n",
    "         'lambda_l2': 10,\n",
    "         'subsample': 0.85,\n",
    "         'learning_rate': 0.01,\n",
    "         'seed': 2018,\n",
    "         'colsample_bytree': 0.5,\n",
    "         'nthread': -1}\n",
    "\n",
    "# x_train, x_val, y_train, y_true = train_test_split(train_X, train_Y, test_size=0.2, random_state=2018)\n",
    "\n",
    "matrix_train = lgb.Dataset(train_X, train_Y)\n",
    "# matrix_test = lgb.Dataset(x_val, y_true)\n",
    "\n",
    "\n",
    "model = lgb.train(params, matrix_train, num_boost_round=50000)\n",
    "# res.append(tpr_weight_funtion(y_true, model.predict(x_val)))\n",
    "\n",
    "predict = model.predict(np.array(test_X))\n",
    "\n",
    "result = pd.DataFrame({\"UID\": test_UID, \"Tag\": predict})\n",
    "result.to_csv(\"./\"+\"lgb_result_time\"+str(int(time.time()))+\".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"*******************************************************************************************************\")\n",
    "# print(\"result:\", result)\n",
    "# print(\"*******************************************************************************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型三：XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_TRAIN = pd.read_csv(\"../data/operation_TRAIN.csv\")\n",
    "transaction_TRAIN = pd.read_csv(\"../data/transaction_TRAIN.csv\")\n",
    "tag_TRAIN = pd.read_csv(\"../data/tag_TRAIN.csv\")                                                          \n",
    "operation_TRAIN = operation_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "transaction_TRAIN = transaction_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "train_X = pd.merge(operation_TRAIN, transaction_TRAIN, on=\"UID\", how=\"outer\")\n",
    "train = pd.merge(train_X, tag_TRAIN, on=\"UID\", how=\"left\")\n",
    "train = train.sort_values(\"UID\")\n",
    "\n",
    "operation_round1 = pd.read_csv(\"../data/operation_round1.csv\")\n",
    "transaction_round1 = pd.read_csv(\"../data/transaction_round1.csv\")                                                                                                                      \n",
    "operation_round1 = operation_round1.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "transaction_round1 = transaction_round1.drop_duplicates([\"UID\"], keep=\"last\")                                                              \n",
    "test_C = pd.merge(operation_round1, transaction_round1, on=\"UID\", how=\"outer\")\n",
    "test_C = test_C.sort_values(\"UID\")\n",
    "\n",
    "del operation_TRAIN\n",
    "del transaction_TRAIN\n",
    "del tag_TRAIN\n",
    "del operation_round1\n",
    "del transaction_round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_C[\"Tag\"] = -1\n",
    "data = pd.concat([train, test_C])\n",
    "data = data.fillna({\"time_x\":\"00:00:00\", \"time_y\":\"00:00:00\"})\n",
    "data = data.fillna(value=-1)\n",
    "data[\"hour_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[0])\n",
    "data[\"minutes_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[1])\n",
    "data[\"hour_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[0])\n",
    "data[\"minutes_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[1])\n",
    "\n",
    "data = data.drop([\"time_x\", \"time_y\"], axis=1)\n",
    "feature = data.columns.drop([\"UID\", \"Tag\"])\n",
    "col_feature = [\"day_x\", \"day_y\", \"bal\", \"trans_amt\"]\n",
    "dis_feature = feature.drop(col_feature)\n",
    "\n",
    "for col in dis_feature:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    \n",
    "train = data[data[\"Tag\"] != -1]\n",
    "train_Y = train[\"Tag\"]\n",
    "train_X = train.drop([\"UID\", \"Tag\"], axis=1)\n",
    "test = data[data[\"Tag\"] == -1]\n",
    "test_UID = test[\"UID\"]\n",
    "test_X = test.drop([\"UID\", \"Tag\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"booster\": \"gbtree\",\n",
    "          \"objective\" : \"binary:logistic\",\n",
    "          \"eval_metric\" : \"auc\",\n",
    "          \"verbose\" : 1, \n",
    "          \"eta\" : 0.01, \n",
    "          \"max_delta_step\" : 5, \n",
    "          \"max_depth\" : 20, \n",
    "          \"alpha\" : 10, \n",
    "          \"lambda\" : 10, \n",
    "          \"gamma\" : 2, \n",
    "          \"subsample\" : 0.8, \n",
    "          \"colsample_bytree\" : 0.8, \n",
    "          \"min_child_weight\" : 5, \n",
    "          \"scale_pos_weight\" : 0.8,\n",
    "          \"n_jobs\" : -1}\n",
    "\n",
    "x_train, x_val, y_train, y_true = train_test_split(train_X, train_Y, test_size=0.2, random_state=2018)\n",
    "\n",
    "matrix_train = xgb.DMatrix(x_train, y_train)\n",
    "matrix_test = xgb.DMatrix(x_val)\n",
    "a_model = xgb.train(params, matrix_train, num_boost_round=3000)\n",
    "y_predict = a_model.predict(matrix_test)\n",
    "\n",
    "\n",
    "d = pd.DataFrame()\n",
    "d['prob'] = list(y_predict)\n",
    "d['y'] = list(y_true)\n",
    "d = d.sort_values(['prob'], ascending=[0])\n",
    "y = d.y\n",
    "PosAll = pd.Series(y).value_counts()[1]\n",
    "NegAll = pd.Series(y).value_counts()[0]\n",
    "pCumsum = d['y'].cumsum()\n",
    "nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "pCumsumPer = pCumsum / PosAll\n",
    "nCumsumPer = nCumsum / NegAll\n",
    "TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "\n",
    "result = 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "print(\"*******************************************************************************************************\")\n",
    "print(\"result:\", result)\n",
    "print(\"*******************************************************************************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = xgb.DMatrix(train_X, train_Y)\n",
    "test_matrix = xgb.DMatrix(test_X)\n",
    "\n",
    "model = xgb.train(params, train_matrix, num_boost_round=3000)\n",
    "predict = model.predict(test_matrix)\n",
    "\n",
    "result = pd.DataFrame({\"UID\": test_UID, \"Tag\": predict})\n",
    "result.to_csv(\"./\"+\"result_time\"+str(int(time.time()))+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型三：lgb+lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@file: lgb+lr.py\n",
    "@company: FinSight\n",
    "@author: Zhao Ming\n",
    "@time: 2018-10-27   14:35:40\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def prepro():\n",
    "    print(\"读取数据...\")\n",
    "    operation_TRAIN = pd.read_csv(\"../data/operation_TRAIN.csv\")\n",
    "    transaction_TRAIN = pd.read_csv(\"../data/transaction_TRAIN.csv\")\n",
    "    tag_TRAIN = pd.read_csv(\"../data/tag_TRAIN.csv\")\n",
    "    operation_TRAIN = operation_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "    transaction_TRAIN = transaction_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "    train_X = pd.merge(operation_TRAIN, transaction_TRAIN, on=\"UID\", how=\"outer\")\n",
    "    train = pd.merge(train_X, tag_TRAIN, on=\"UID\", how=\"left\")\n",
    "    train = train.sort_values(\"UID\")\n",
    "\n",
    "    operation_round1 = pd.read_csv(\"../data/operation_round1.csv\")\n",
    "    transaction_round1 = pd.read_csv(\"../data/transaction_round1.csv\")\n",
    "    operation_round1 = operation_round1.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "    transaction_round1 = transaction_round1.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "    test_C = pd.merge(operation_round1, transaction_round1, on=\"UID\", how=\"outer\")\n",
    "    test_C = test_C.sort_values(\"UID\")\n",
    "    test_id = test_C.UID\n",
    "\n",
    "    print(\"读取结束...\")\n",
    "\n",
    "    test_C[\"Tag\"] = -1\n",
    "    data = pd.concat([train, test_C])\n",
    "    data[\"time_x\"] = data[\"time_x\"].fillna(value=\"00:00:00\")\n",
    "    data[\"time_y\"] = data[\"time_y\"].fillna(value=\"00:00:00\")\n",
    "    data[\"day_x\"] = data[\"day_x\"].fillna(value=0)\n",
    "    data[\"day_y\"] = data[\"day_y\"].fillna(value=30)\n",
    "    data = data.fillna(value=-99999)\n",
    "\n",
    "    data[\"hour_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[0])\n",
    "    data[\"minutes_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[1])\n",
    "    data[\"hour_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[0])\n",
    "    data[\"minutes_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[1])\n",
    "\n",
    "    data[\"day_y\"] = data[\"day_y\"] + 30\n",
    "    data[\"day_sub\"] = data[\"day_y\"] - data[\"day_x\"]\n",
    "    data[\"hour_sub\"] = (data[\"hour_y\"].astype(int) +24) - data[\"hour_x\"].astype(int)\n",
    "    data[\"minutes_sub\"] = (data[\"minutes_y\"].astype(int) +60) - data[\"minutes_x\"].astype(int)\n",
    "\n",
    "    feature = data.columns.drop([\"UID\", \"Tag\"])\n",
    "    col_feature = [\"day_x\", \"hour_sub\", \"minutes_sub\", \"bal\", \"trans_amt\"]\n",
    "    dis_feature = feature.drop(col_feature)\n",
    "\n",
    "    return data, col_feature, dis_feature, test_id\n",
    "\n",
    "\n",
    "def gbdt_lr_predict(data, dis_feature, con_feature, test_id):\n",
    "    \"\"\"离散特征one-hot\"\"\"\n",
    "    print(\"开始one-hot...\")\n",
    "    for col in dis_feature:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    print(\"one-hot结束\")\n",
    "\n",
    "    train = data[data[\"Tag\"] != -1]\n",
    "    target = train.pop(\"Tag\")\n",
    "    test = data[data[\"Tag\"] == -1]\n",
    "    test.drop([\"Tag\"], axis=1, inplace=True)\n",
    "\n",
    "    print(\"划分数据集...\")\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=2018)\n",
    "\n",
    "    print(\"开始训练gbdt...\")\n",
    "    gbm = lgb.LGBMRegressor(boosting_type='gbdt',\n",
    "                            objective='binary',\n",
    "                            metric= 'auc',\n",
    "                            min_child_weight= 1.5,\n",
    "                            num_leaves= 2 ** 5,\n",
    "                            lambda_l2= 10,\n",
    "                            subsample=0.85,\n",
    "                            learning_rate= 0.01,\n",
    "                            seed=2018,\n",
    "                            colsample_bytree=0.5,\n",
    "                            n_estimators=2000,\n",
    "                            n_jobs= -1)\n",
    "    gbm.fit(x_train, y_train,\n",
    "            eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "            eval_names=[\"train\", \"val\"],\n",
    "            eval_metric=\"binary_logloss\")\n",
    "    model = gbm.booster_\n",
    "    print(\"训练得到叶子数...\")\n",
    "    gbdt_feats_train = model.predict(train, pred_leaf=True)\n",
    "    gbdt_feats_test = model.predict(test, pred_leaf=True)\n",
    "    gbdt_feats_name = [\"gbdt_leaf_\" + str(i) for i in range(gbdt_feats_train.shape[1])]\n",
    "    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)\n",
    "    df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns=gbdt_feats_name)\n",
    "\n",
    "    print(\"构造新的数据集...\")\n",
    "    train = pd.concat([train, df_train_gbdt_feats], axis=1)\n",
    "    test = pd.concat([test, df_test_gbdt_feats], axis=1)\n",
    "    train_len = train.shape[0]\n",
    "    data = pd.concat([train, test])\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    \"\"\"连续特征归一化\"\"\"\n",
    "    print(\"开始归一化...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in con_feature:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    print(\"归一化结束\")\n",
    "\n",
    "    \"\"\"叶子数One-Hot\"\"\"\n",
    "    print(\"开始one-hot...\")\n",
    "    for col in gbdt_feats_name:\n",
    "        print(\"this is feature:\", col)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    print(\"one-hot结束\")\n",
    "\n",
    "    train = data[:train_len]\n",
    "    test = data[train_len:]\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.3, random_state=2018)\n",
    "    print(\"开始训练lr...\")\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:, 1])\n",
    "    print(\"tr-logloss:\", tr_logloss)\n",
    "    val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    print(\"val-logloss:\", val_logloss)\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=lw, label=\"ROC curve (area = %0.5f)\" % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"gbdt_lr\" + \"ROC\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"开始预测...\")\n",
    "    y_pred = lr.predict_proba(test)[:, 1]\n",
    "    print(\"写入结果...\")\n",
    "    submission = pd.DataFrame({\"UID\": test_id, \"Tag\": y_pred})\n",
    "    submission.to_csv(\"../submission/\"+str(int(time.time()))+\".csv\", index=False)\n",
    "    print(\"结束\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"time start:\\n\")\n",
    "    print(time.clock() / 60)\n",
    "\n",
    "    data, col_feature, dis_feature, test_id = prepro()\n",
    "    gbdt_lr_predict(data, dis_feature, col_feature, test_id)\n",
    "\n",
    "    print('time end')\n",
    "    print(time.clock() / 60)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型四：xgb+lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@file: lgb+lr.py\n",
    "@company: FinSight\n",
    "@author: Zhao Ming\n",
    "@time: 2018-11-01   15:20:10\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def prepro():\n",
    "    print(\"读取数据...\")\n",
    "    operation_TRAIN = pd.read_csv(\"../data/operation_TRAIN.csv\")\n",
    "    transaction_TRAIN = pd.read_csv(\"../data/transaction_TRAIN.csv\")\n",
    "    tag_TRAIN = pd.read_csv(\"../data/tag_TRAIN.csv\")\n",
    "    operation_TRAIN = operation_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "    transaction_TRAIN = transaction_TRAIN.drop_duplicates(\"UID\", keep=\"last\")\n",
    "    train_X = pd.merge(operation_TRAIN, transaction_TRAIN, on=\"UID\", how=\"outer\")\n",
    "    train = pd.merge(train_X, tag_TRAIN, on=\"UID\", how=\"left\")\n",
    "    train = train.sort_values(\"UID\")\n",
    "\n",
    "    operation_round1 = pd.read_csv(\"../data/operation_round1.csv\")\n",
    "    transaction_round1 = pd.read_csv(\"../data/transaction_round1.csv\")\n",
    "    operation_round1 = operation_round1.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "    transaction_round1 = transaction_round1.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "    test_C = pd.merge(operation_round1, transaction_round1, on=\"UID\", how=\"outer\")\n",
    "    test_C = test_C.sort_values(\"UID\")\n",
    "    test_id = test_C.UID\n",
    "\n",
    "    print(\"读取结束...\")\n",
    "\n",
    "    test_C[\"Tag\"] = -1\n",
    "    data = pd.concat([train, test_C])\n",
    "    data[\"time_x\"] = data[\"time_x\"].fillna(value=\"00:00:00\")\n",
    "    data[\"time_y\"] = data[\"time_y\"].fillna(value=\"00:00:00\")\n",
    "    data[\"day_x\"] = data[\"day_x\"].fillna(value=0)\n",
    "    data[\"day_y\"] = data[\"day_y\"].fillna(value=30)\n",
    "    data = data.fillna(value=-99999)\n",
    "\n",
    "    data[\"hour_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[0])\n",
    "    data[\"minutes_x\"] = data.time_x.apply(lambda  x: x.split(\":\")[1])\n",
    "    data[\"hour_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[0])\n",
    "    data[\"minutes_y\"] = data.time_y.apply(lambda  x: x.split(\":\")[1])\n",
    "\n",
    "    data[\"day_y\"] = data[\"day_y\"] + 30\n",
    "    data[\"day_sub\"] = data[\"day_y\"] - data[\"day_x\"]\n",
    "    data[\"hour_sub\"] = (data[\"hour_y\"].astype(int) +24) - data[\"hour_x\"].astype(int)\n",
    "    data[\"minutes_sub\"] = (data[\"minutes_y\"].astype(int) +60) - data[\"minutes_x\"].astype(int)\n",
    "\n",
    "    feature = data.columns.drop([\"UID\", \"Tag\"])\n",
    "    col_feature = [\"day_x\", \"hour_sub\", \"minutes_sub\", \"bal\", \"trans_amt\"]\n",
    "    dis_feature = feature.drop(col_feature)\n",
    "\n",
    "    return data, col_feature, dis_feature, test_id\n",
    "\n",
    "\n",
    "def gbdt_lr_predict(data, dis_feature, con_feature, test_id):\n",
    "    \"\"\"离散特征one-hot\"\"\"\n",
    "    print(\"开始one-hot...\")\n",
    "    for col in dis_feature:\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    print(\"one-hot结束\")\n",
    "\n",
    "    train = data[data[\"Tag\"] != -1]\n",
    "    target = train.pop(\"Tag\")\n",
    "    test = data[data[\"Tag\"] == -1]\n",
    "    test.drop([\"Tag\"], axis=1, inplace=True)\n",
    "\n",
    "    print(\"划分数据集...\")\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=2018)\n",
    "\n",
    "    print(\"开始训练gbdt...\")\n",
    "\n",
    "    # lgb模型\n",
    "    # lgbm = lgb.LGBMClassifier(booster=\"gbtree\",\n",
    "    #                         subsample=0.8,\n",
    "    #                         min_child_weight=0.2,\n",
    "    #                         colsample_bytree=0.8,\n",
    "    #                         num_leaves=100,\n",
    "    #                         max_depth=20,\n",
    "    #                         reg_lambda=20,\n",
    "    #                         reg_alpha=10,\n",
    "    #                         learning_rate=0.01,\n",
    "    #                         n_estimators=2000,\n",
    "    #                         n_jobs=-1)\n",
    "    # lgbm.fit(x_train, y_train,\n",
    "    #         eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "    #         eval_names=[\"train\", \"val\"],\n",
    "    #         eval_metric=\"binary_logloss\")\n",
    "    # model = lgbm.booster_\n",
    "    \n",
    "    # xgb模型\n",
    "    params = {\"booster\": \"gbtree\",\n",
    "              \"objective\": \"binary:logistic\",\n",
    "              \"eval_metric\": \"auc\",\n",
    "              \"verbose\": 1,\n",
    "              \"eta\": 0.01,\n",
    "              \"max_delta_step\": 5,\n",
    "              \"max_depth\": 20,\n",
    "              \"alpha\": 10,\n",
    "              \"lambda\": 10,\n",
    "              \"gamma\": 2,\n",
    "              \"subsample\": 0.8,\n",
    "              \"colsample_bytree\": 0.8,\n",
    "              \"min_child_weight\": 5,\n",
    "              \"scale_pos_weight\": 0.8,\n",
    "              \"n_jobs\": -1}\n",
    "    matrix_train = xgb.DMatrix(x_train, y_train)\n",
    "    Matrix_Train = xgb.DMatrix(train, target)\n",
    "#     matrix_val = xgb.DMatrix(x_val)\n",
    "    matrix_test = xgb.DMatrix(test)\n",
    "    model = xgb.train(params, matrix_train, num_boost_round=3000)\n",
    "\n",
    "\n",
    "    print(\"训练得到叶子数...\")\n",
    "    gbdt_feats_train = model.predict(Matrix_Train, pred_leaf=True)\n",
    "    gbdt_feats_test = model.predict(matrix_test, pred_leaf=True)\n",
    "    gbdt_feats_name = [\"gbdt_leaf_\" + str(i) for i in range(gbdt_feats_train.shape[1])]\n",
    "    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)\n",
    "    df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns=gbdt_feats_name)\n",
    "\n",
    "    print(\"构造新的数据集...\")\n",
    "    train = pd.concat([train, df_train_gbdt_feats], axis=1)\n",
    "    test = pd.concat([test, df_test_gbdt_feats], axis=1)\n",
    "    train_len = train.shape[0]\n",
    "    data = pd.concat([train, test])\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    \"\"\"连续特征归一化\"\"\"\n",
    "    print(\"开始归一化...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    for col in con_feature:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    print(\"归一化结束\")\n",
    "\n",
    "    \"\"\"叶子数One-Hot\"\"\"\n",
    "    print(\"开始one-hot...\")\n",
    "    for col in gbdt_feats_name:\n",
    "        print(\"this is feature:\", col)\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        data[col] = lbl.fit_transform(list(data[col].values))\n",
    "    print(\"one-hot结束\")\n",
    "\n",
    "    train = data[:train_len]\n",
    "    test = data[train_len:]\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.3, random_state=2018)\n",
    "    print(\"开始训练lr...\")\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    tr_logloss = log_loss(y_train, lr.predict_proba(x_train)[:, 1])\n",
    "    print(\"tr-logloss:\", tr_logloss)\n",
    "    val_logloss = log_loss(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    print(\"val-logloss:\", val_logloss)\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_val, lr.predict_proba(x_val)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=lw, label=\"ROC curve (area = %0.5f)\" % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"gbdt_lr\" + \"ROC\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"开始预测...\")\n",
    "    y_pred = lr.predict_proba(test)[:, 1]\n",
    "    print(\"写入结果...\")\n",
    "    submission = pd.DataFrame({\"UID\": test_id, \"Tag\": y_pred})\n",
    "    submission.to_csv(\"../submission/\"+str(int(time.time()))+\".csv\", index=False)\n",
    "    print(\"结束\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"time start:\\n\")\n",
    "    print(time.clock() / 60)\n",
    "\n",
    "    data, col_feature, dis_feature, test_id = prepro()\n",
    "    gbdt_lr_predict(data, dis_feature, col_feature, test_id)\n",
    "\n",
    "    print('time end')\n",
    "    print(time.clock() / 60)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
