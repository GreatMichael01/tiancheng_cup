{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取operation_train_new，和operation_round1_new数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_train_new = pd.read_csv(\"../data/operation_train_new.csv\")\n",
    "operation_round1_new = pd.read_csv(\"../data/operation_round1_new.csv\")\n",
    "operation = pd.concat([operation_train_new, operation_round1_new], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对operation_train_new数据的时间进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation[\"time\"] = operation[\"time\"].fillna(value=\"00:00:00\")\n",
    "operation[\"hour\"] = operation.time.apply(lambda  x: x.split(\":\")[0]).astype(int)\n",
    "operation[\"minutes\"] = operation.time.apply(lambda  x: x.split(\":\")[1]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "operation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = operation.drop([\"time\"], axis=1)\n",
    "feature = operation.columns.drop([\"UID\"])\n",
    "\n",
    "print(\"one-hot...\")\n",
    "for col in feature:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    operation[col] = lbl.fit_transform(list(operation[col].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对operation_new数据开发新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_operation_UID = pd.DataFrame(operation.drop_duplicates(\"UID\").UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_new = operation\n",
    "gby=operation_new.loc[:,['UID', 'day', 'mode', 'success', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "                   'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code', \n",
    "                   'ip1_sub', 'ip2_sub', 'hour', 'minutes']].groupby([\"UID\"],as_index=False)\n",
    "\n",
    "operation_new_sum = gby.sum()\n",
    "operation_new_sum.columns = ['UID', 'day_sum', 'mode_sum', 'success_sum', 'os_sum', 'version_sum', 'device1_sum',\n",
    "                         'device2_sum', 'device_code1_sum', 'device_code2_sum', 'device_code3_sum', 'mac1_sum',\n",
    "                         'mac2_sum', 'ip1_sum', 'ip2_sum', 'wifi_sum', 'geo_code_sum', 'ip1_sub_sum', 'ip2_sub_sum',\n",
    "                         'hour_sum', 'minutes_sum']\n",
    "data1=pd.merge(operation_new, operation_new_sum, how='left', on = \"UID\")\n",
    "\n",
    "operation_new_count = gby.count()\n",
    "operation_new_count.columns = ['UID', 'day_count', 'mode_count', 'success_count', 'os_count', 'version_count', 'device1_count',\n",
    "                         'device2_count', 'device_code1_count', 'device_code2_count', 'device_code3_count', 'mac1_count',\n",
    "                         'mac2_count', 'ip1_count', 'ip2_count', 'wifi_count', 'geo_code_count', 'ip1_sub_count', 'ip2_sub_count',\n",
    "                         'hour_count', 'minutes_count']\n",
    "data1 = pd.merge(data1, operation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "operation_new_max = gby.max()\n",
    "operation_new_max.columns = ['UID', 'day_max', 'mode_max', 'success_max', 'os_max', 'version_max', 'device1_max',\n",
    "                         'device2_max', 'device_code1_max', 'device_code2_max', 'device_code3_max', 'mac1_max',\n",
    "                         'mac2_max', 'ip1_max', 'ip2_max', 'wifi_max', 'geo_code_max', 'ip1_sub_max', 'ip2_sub_max',\n",
    "                         'hour_max', 'minutes_max']\n",
    "data1 = pd.merge(data1, operation_new_max, how=\"left\", on=\"UID\")\n",
    "\n",
    "operation_new_min = gby.min()\n",
    "operation_new_min.columns = ['UID', 'day_min', 'mode_min', 'success_min', 'os_min', 'version_min', 'device1_min',\n",
    "                         'device2_min', 'device_code1_min', 'device_code2_min', 'device_code3_min', 'mac1_min',\n",
    "                         'mac2_min', 'ip1_min', 'ip2_min', 'wifi_min', 'geo_code_min', 'ip1_sub_min', 'ip2_sub_min',\n",
    "                         'hour_min', 'minutes_min']\n",
    "data1 = pd.merge(data1, operation_new_min, how=\"left\", on=\"UID\")\n",
    "\n",
    "operation_new_mean = gby.mean()\n",
    "operation_new_mean.columns = ['UID', 'day_mean', 'mode_mean', 'success_mean', 'os_mean', 'version_mean', 'device1_mean',\n",
    "                         'device2_mean', 'device_code1_mean', 'device_code2_mean', 'device_code3_mean', 'mac1_mean',\n",
    "                         'mac2_mean', 'ip1_mean', 'ip2_mean', 'wifi_mean', 'geo_code_mean', 'ip1_sub_mean', 'ip2_sub_mean',\n",
    "                         'hour_mean', 'minutes_mean']\n",
    "data1 = pd.merge(data1, operation_new_mean, how=\"left\", on=\"UID\")\n",
    "\n",
    "operation_new_median = gby.median()\n",
    "operation_new_median.columns = ['UID', 'day_median', 'mode_median', 'success_median', 'os_median', 'version_median', 'device1_median',\n",
    "                         'device2_median', 'device_code1_median', 'device_code2_median', 'device_code3_median', 'mac1_median',\n",
    "                         'mac2_median', 'ip1_median', 'ip2_median', 'wifi_median', 'geo_code_median', 'ip1_sub_median', 'ip2_sub_median',\n",
    "                         'hour_median', 'minutes_median']\n",
    "data1 = pd.merge(data1, operation_new_median, how=\"left\", on=\"UID\")\n",
    "\n",
    "operation_new_std = gby.std()\n",
    "operation_new_std.columns = ['UID', 'day_std', 'mode_std', 'success_std', 'os_std', 'version_std', 'device1_std',\n",
    "                         'device2_std', 'device_code1_std', 'device_code2_std', 'device_code3_std', 'mac1_std',\n",
    "                         'mac2_std', 'ip1_std', 'ip2_std', 'wifi_std', 'geo_code_std', 'ip1_sub_std', 'ip2_sub_std',\n",
    "                         'hour_std', 'minutes_std']\n",
    "data1 = pd.merge(data1, operation_new_std, how=\"left\", on=\"UID\")\n",
    "\n",
    "operation_new_var = gby.var()\n",
    "operation_new_var.columns = ['UID', 'day_var', 'mode_var', 'success_var', 'os_var', 'version_var', 'device1_var',\n",
    "                         'device2_var', 'device_code1_var', 'device_code2_var', 'device_code3_var', 'mac1_var',\n",
    "                         'mac2_var', 'ip1_var', 'ip2_var', 'wifi_var', 'geo_code_var', 'ip1_sub_var', 'ip2_sub_var',\n",
    "                         'hour_var', 'minutes_var']\n",
    "data1 = pd.merge(data1, operation_new_var, how=\"left\", on=\"UID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.drop_duplicates(\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del operation_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对operation_new补充新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_opera_new = operation_new.drop_duplicates(\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_opera_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同一用户在同一天内操作的数据信息\n",
    "\n",
    "operation_new = operation\n",
    "daygby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "                   'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code', \n",
    "                   'ip1_sub', 'ip2_sub']].groupby([\"day\"],as_index=False)\n",
    "dayoperation_new_count = daygby.count()\n",
    "dayoperation_new_count.columns = ['UID', 'day_day_count',  'day_os_count', 'day_version_count', 'day_device1_count',\n",
    "                                  'day_device2_count', 'day_device_code1_count', 'day_device_code2_count', 'day_device_code3_count',\n",
    "                                  'day_mac1_count','day_mac2_count', 'day_ip1_count', 'day_ip2_count', 'day_wifi_count', \n",
    "                                  'day_geo_code_count','day_ip1_sub_count','day_ip2_sub_count']\n",
    "data1_1 = pd.merge(data1, dayoperation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "mac1gby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "                   'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code', \n",
    "                   'ip1_sub', 'ip2_sub']].groupby([\"mac1\"],as_index=False)\n",
    "mac1operation_new_count = mac1gby.count()\n",
    "mac1operation_new_count.columns = ['UID', 'mac1_day_count',  'mac1_os_count', 'mac1_version_count', 'mac1_device1_count',\n",
    "                                  'mac1_device2_count', 'mac1_device_code1_count', 'mac1_device_code2_count', 'mac1_device_code3_count',\n",
    "                                  'mac1_mac1_count','mac1_mac2_count', 'mac1_ip1_count', 'mac1_ip2_count', 'mac1_wifi_count', \n",
    "                                  'mac1_geo_code_count','mac1_ip1_sub_count','mac1_ip2_sub_count']\n",
    "data1_2 = pd.merge(data1_1, mac1operation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "ip1gby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "                   'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code', \n",
    "                   'ip1_sub', 'ip2_sub']].groupby([\"ip1\"],as_index=False)\n",
    "ip1operation_new_count = ip1gby.count()\n",
    "ip1operation_new_count.columns = ['UID', 'ip1_day_count',  'ip1_os_count', 'ip1_version_count', 'ip1_device1_count',\n",
    "                                  'ip1_device2_count', 'ip1_device_code1_count', 'ip1_device_code2_count', 'ip1_device_code3_count',\n",
    "                                  'ip1_mac1_count','ip1_mac2_count', 'ip1_ip1_count', 'ip1_ip2_count', 'ip1_wifi_count', \n",
    "                                  'ip1_geo_code_count','ip1_ip1_sub_count','ip1_ip2_sub_count']\n",
    "data1_3 = pd.merge(data1_2, ip1operation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "# mac2gby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "#                    'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code', \n",
    "#                    'ip1_sub', 'ip2_sub']].groupby([\"mac2\"],as_index=False)\n",
    "# mac2operation_new_count = mac2gby.count()\n",
    "# mac2operation_new_count.columns = ['UID', 'mac2_day_count',  'mac2_os_count', 'mac2_version_count', 'mac2_device1_count',\n",
    "#                                   'mac2_device2_count', 'mac2_device_code1_count', 'mac2_device_code2_count', 'mac2_device_code3_count',\n",
    "#                                   'mac2_mac1_count','mac2_mac2_count', 'mac2_ip1_count', 'mac2_ip2_count', 'mac2_wifi_count', \n",
    "#                                   'mac2_geo_code_count','mac2_ip1_sub_count','mac2_ip2_sub_count']\n",
    "# data1_4 = pd.merge(data1_3, mac2operation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "# ip2gby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "#                    'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code', \n",
    "#                    'ip1_sub', 'ip2_sub']].groupby([\"ip2\"],as_index=False)\n",
    "# ip2operation_new_count = ip2gby.count()\n",
    "# ip2operation_new_count.columns = ['UID', 'ip2_day_count',  'ip2_os_count', 'ip2_version_count', 'ip2_device1_count',\n",
    "#                                   'ip2_device2_count', 'ip2_device_code1_count', 'ip2_device_code2_count', 'ip2_device_code3_count',\n",
    "#                                   'ip2_mac1_count','ip2_mac2_count', 'ip2_ip1_count', 'ip2_ip2_count', 'ip2_wifi_count', \n",
    "#                                   'ip2_geo_code_count','ip2_ip1_sub_count','ip2_ip2_sub_count']\n",
    "# data1_5 = pd.merge(data1_4, ip2operation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "# wifigby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "#                    'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code',\n",
    "#                    'ip1_sub', 'ip2_sub']].groupby([\"wifi\"],as_index=False)\n",
    "# wifioperation_new_count = wifigby.count()\n",
    "# wifioperation_new_count.columns = ['UID', 'wifi_day_count',  'wifi_os_count', 'wifi_version_count', 'wifi_device1_count',\n",
    "#                                   'wifi_device2_count', 'wifi_device_code1_count', 'wifi_device_code2_count', 'wifi_device_code3_count',\n",
    "#                                   'wifi_mac1_count','wifi_mac2_count', 'wifi_ip1_count', 'wifi_ip2_count', 'wifi_wifi_count',\n",
    "#                                   'wifi_geo_code_count','wifi_ip1_sub_count','wifi_ip2_sub_count']\n",
    "# data1_6 = pd.merge(data1_5, wifioperation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "# geo_codegby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "#                    'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code',\n",
    "#                    'ip1_sub', 'ip2_sub']].groupby([\"geo_code\"],as_index=False)\n",
    "# geo_codeoperation_new_count = geo_codegby.count()\n",
    "# geo_codeoperation_new_count.columns = ['UID', 'geo_code_day_count',  'geo_code_os_count', 'geo_code_version_count', 'geo_code_device1_count',\n",
    "#                                   'geo_code_device2_count', 'geo_code_device_code1_count', 'geo_code_device_code2_count', 'geo_code_device_code3_count',\n",
    "#                                   'geo_code_mac1_count','geo_code_mac2_count', 'geo_code_ip1_count', 'geo_code_ip2_count', 'geo_code_wifi_count',\n",
    "#                                   'geo_code_geo_code_count','geo_code_ip1_sub_count','geo_code_ip2_sub_count']\n",
    "# data1_7 = pd.merge(data1_6, geo_codeoperation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "# ip1_subgby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "#                    'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code',\n",
    "#                    'ip1_sub', 'ip2_sub']].groupby([\"ip1_sub\"],as_index=False)\n",
    "# ip1_suboperation_new_count = ip1_subgby.count()\n",
    "# ip1_suboperation_new_count.columns = ['UID', 'ip1_sub_day_count',  'ip1_sub_os_count', 'ip1_sub_version_count', 'ip1_sub_device1_count',\n",
    "#                                   'ip1_sub_device2_count', 'ip1_sub_device_code1_count', 'ip1_sub_device_code2_count', 'ip1_sub_device_code3_count',\n",
    "#                                   'ip1_sub_mac1_count','ip1_sub_mac2_count', 'ip1_sub_ip1_count', 'ip1_sub_ip2_count', 'ip1_sub_wifi_count',\n",
    "#                                   'ip1_sub_geo_code_count','ip1_sub_ip1_sub_count','ip1_sub_ip2_sub_count']\n",
    "# data1_8 = pd.merge(data1_7, ip1_suboperation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "# ip2_subgby=operation.loc[:,['UID', 'day', 'os', 'version','device1', 'device2', 'device_code1', 'device_code2',\n",
    "#                    'device_code3', 'mac1', 'mac2', 'ip1', 'ip2', 'wifi','geo_code',\n",
    "#                    'ip1_sub', 'ip2_sub']].groupby([\"ip2_sub\"],as_index=False)\n",
    "# ip2_suboperation_new_count = ip2_subgby.count()\n",
    "# ip2_suboperation_new_count.columns = ['UID', 'ip2_sub_day_count',  'ip2_sub_os_count', 'ip2_sub_version_count', 'ip2_sub_device1_count',\n",
    "#                                   'ip2_sub_device2_count', 'ip2_sub_device_code1_count', 'ip2_sub_device_code2_count', 'ip2_sub_device_code3_count',\n",
    "#                                   'ip2_sub_mac1_count','ip2_sub_mac2_count', 'ip2_sub_ip1_count', 'ip2_sub_ip2_count', 'ip2_sub_wifi_count',\n",
    "#                                   'ip2_sub_geo_code_count','ip2_sub_ip1_sub_count','ip2_sub_ip2_sub_count']\n",
    "# data1_9 = pd.merge(data1_8, ip2_suboperation_new_count, how=\"left\", on=\"UID\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取transaction_train_new，和transaction_round1_new数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_train_new = pd.read_csv(\"../data/transaction_train_new.csv\")\n",
    "transaction_round1_new = pd.read_csv(\"../data/transaction_round1_new.csv\")\n",
    "transaction = pd.concat([transaction_train_new, transaction_round1_new], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对transaction_train_new数据进行时间处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction[\"time\"] = transaction[\"time\"].fillna(value=\"00:00:00\")\n",
    "transaction[\"hour\"] = transaction.time.apply(lambda  x: x.split(\":\")[0]).astype(int)\n",
    "transaction[\"minutes\"] = transaction.time.apply(lambda  x: x.split(\":\")[1]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction = transaction.drop([\"time\"], axis=1)\n",
    "feature = transaction.columns.drop([\"UID\"])\n",
    "con_feature = [\"bal\", \"trans_amt\"]\n",
    "dis_feature = feature.drop(con_feature)\n",
    "\n",
    "print(\"one-hot...\")\n",
    "for col in dis_feature:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    transaction[col] = lbl.fit_transform(list(transaction[col].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_trans_new_UID = pd.DataFrame(transaction.drop_duplicates(\"UID\").UID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transaction_new = transaction\n",
    "gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1', \n",
    "                           'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "                           'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "                           'ip1_sub', 'hour', 'minutes']].groupby([\"UID\"],as_index=False)\n",
    "\n",
    "transaction_new_sum = gby.sum()\n",
    "transaction_new_sum.columns = ['UID', 'channel_sum', 'day_sum', 'trans_amt_sum', 'amt_src1_sum', 'merchant_sum', 'code1_sum', \n",
    "                               'code2_sum', 'trans_type1_sum', 'acc_id1_sum', 'device_code1_sum', 'device_code2_sum', \n",
    "                               'device_code3_sum', 'device1_sum', 'device2_sum', 'mac1_sum', 'ip1_sum', 'bal_sum', 'amt_src2_sum',\n",
    "                               'acc_id2_sum', 'acc_id3_sum', 'geo_code_sum', 'trans_type2_sum', 'market_code_sum', 'market_type_sum', \n",
    "                               'p1_sub_sum', 'hour_sum', 'minutes_sum']\n",
    "data2=pd.merge(transaction_new, transaction_new_sum, how='left', on = \"UID\")\n",
    "\n",
    "transaction_count = gby.count()\n",
    "transaction_count.columns = ['UID', 'channel_count', 'day_count', 'trans_amt_count', 'amt_src1_count', 'merchant_count', 'code1_count', \n",
    "                               'code2_count', 'trans_type1_count', 'acc_id1_count', 'device_code1_count', 'device_code2_count', \n",
    "                               'device_code3_count', 'device1_count', 'device2_count', 'mac1_count', 'ip1_count', 'bal_count', 'amt_src2_count',\n",
    "                               'acc_id2_count', 'acc_id3_count', 'geo_code_count', 'trans_type2_count', 'market_code_count', 'market_type_count', \n",
    "                               'p1_sub_count', 'hour_count', 'minutes_count']\n",
    "data2=pd.merge(data2, transaction_count, how='left', on = \"UID\")\n",
    "\n",
    "transaction_max = gby.max()\n",
    "transaction_max.columns = ['UID', 'channel_max', 'day_max', 'trans_amt_max', 'amt_src1_max', 'merchant_max', 'code1_max', \n",
    "                               'code2_max', 'trans_type1_max', 'acc_id1_max', 'device_code1_max', 'device_code2_max', \n",
    "                               'device_code3_max', 'device1_max', 'device2_max', 'mac1_max', 'ip1_max', 'bal_max', 'amt_src2_max',\n",
    "                               'acc_id2_max', 'acc_id3_max', 'geo_code_max', 'trans_type2_max', 'market_code_max', 'market_type_max', \n",
    "                               'p1_sub_max', 'hour_max', 'minutes_max']\n",
    "data2=pd.merge(data2, transaction_max, how='left', on = \"UID\")\n",
    "\n",
    "transaction_min = gby.min()\n",
    "transaction_min.columns = ['UID', 'channel_min', 'day_min', 'trans_amt_min', 'amt_src1_min', 'merchant_min', 'code1_min', \n",
    "                               'code2_min', 'trans_type1_min', 'acc_id1_min', 'device_code1_min', 'device_code2_min', \n",
    "                               'device_code3_min', 'device1_min', 'device2_min', 'mac1_min', 'ip1_min', 'bal_min', 'amt_src2_min',\n",
    "                               'acc_id2_min', 'acc_id3_min', 'geo_code_min', 'trans_type2_min', 'market_code_min', 'market_type_min', \n",
    "                               'p1_sub_min', 'hour_min', 'minutes_min']\n",
    "data2=pd.merge(data2, transaction_min, how='left', on = \"UID\")\n",
    "\n",
    "transaction_mean = gby.mean()\n",
    "transaction_mean.columns = ['UID', 'channel_mean', 'day_mean', 'trans_amt_mean', 'amt_src1_mean', 'merchant_mean', 'code1_mean', \n",
    "                               'code2_mean', 'trans_type1_mean', 'acc_id1_mean', 'device_code1_mean', 'device_code2_mean', \n",
    "                               'device_code3_mean', 'device1_mean', 'device2_mean', 'mac1_mean', 'ip1_mean', 'bal_mean', 'amt_src2_mean',\n",
    "                               'acc_id2_mean', 'acc_id3_mean', 'geo_code_mean', 'trans_type2_mean', 'market_code_mean', 'market_type_mean', \n",
    "                               'p1_sub_mean', 'hour_mean', 'minutes_mean']\n",
    "data2=pd.merge(data2, transaction_mean, how='left', on = \"UID\")\n",
    "\n",
    "transaction_median = gby.median()\n",
    "transaction_median.columns = ['UID', 'channel_median', 'day_median', 'trans_amt_median', 'amt_src1_median', 'merchant_median', 'code1_median', \n",
    "                               'code2_median', 'trans_type1_median', 'acc_id1_median', 'device_code1_median', 'device_code2_median', \n",
    "                               'device_code3_median', 'device1_median', 'device2_median', 'mac1_median', 'ip1_median', 'bal_median', 'amt_src2_median',\n",
    "                               'acc_id2_median', 'acc_id3_median', 'geo_code_median', 'trans_type2_median', 'market_code_median', 'market_type_median', \n",
    "                               'p1_sub_median', 'hour_median', 'minutes_median']\n",
    "data2=pd.merge(data2, transaction_median, how='left', on = \"UID\")\n",
    "\n",
    "transaction_std = gby.std()\n",
    "transaction_std.columns = ['UID', 'channel_std', 'day_std', 'trans_amt_std', 'amt_src1_std', 'merchant_std', 'code1_std', \n",
    "                               'code2_std', 'trans_type1_std', 'acc_id1_std', 'device_code1_std', 'device_code2_std', \n",
    "                               'device_code3_std', 'device1_std', 'device2_std', 'mac1_std', 'ip1_std', 'bal_std', 'amt_src2_std',\n",
    "                               'acc_id2_std', 'acc_id3_std', 'geo_code_std', 'trans_type2_std', 'market_code_std', 'market_type_std', \n",
    "                               'p1_sub_std', 'hour_std', 'minutes_std']\n",
    "data2=pd.merge(data2, transaction_std, how='left', on = \"UID\")\n",
    "\n",
    "transaction_var = gby.var()\n",
    "transaction_var.columns = ['UID', 'channel_var', 'day_var', 'trans_amt_var', 'amt_src1_var', 'merchant_var', 'code1_var', \n",
    "                               'code2_var', 'trans_type1_var', 'acc_id1_var', 'device_code1_var', 'device_code2_var', \n",
    "                               'device_code3_var', 'device1_var', 'device2_var', 'mac1_var', 'ip1_var', 'bal_var', 'amt_src2_var',\n",
    "                               'acc_id2_var', 'acc_id3_var', 'geo_code_var', 'trans_type2_var', 'market_code_var', 'market_type_var', \n",
    "                               'p1_sub_var', 'hour_var', 'minutes_var']\n",
    "data2=pd.merge(data2, transaction_var, how='left', on = \"UID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop_duplicates(\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对transaction_new补充特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_new = transaction\n",
    "dup_trans_new = transaction_new.drop_duplicates(\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_trans_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同一用户在同一天内操作的数据信息\n",
    "transaction_new = transaction\n",
    "gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1', \n",
    "                           'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "                           'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "                           'ip1_sub']].groupby([\"day\"],as_index=False)\n",
    "\n",
    "daytransaction_new_count = gby.count()\n",
    "daytransaction_new_count.columns = ['UID','day_channel_count', 'day_day_count',  'day_trans_amt_count', 'day_amt_src1_count', 'day_merchant_count',\n",
    "                                    'day_code1_count','day_code2_count', 'day_trans_type1_count', 'day_acc_id1_count','day_device_code1_count',\n",
    "                                    'day_device_code2_count','day_device_code3_count', 'day_device1_count', 'day_device2_count', \n",
    "                                    'day_mac1_count','day_ip1_count','day_bal_count', 'day_amt_src2_count', 'day_acc_id2_count', \n",
    "                                    'day_acc_id3_count', 'day_geo_code_count', 'day_trans_type2_count', 'day_market_code_count',\n",
    "                                    'day_market_type_count', 'day_ip1_sub_count']\n",
    "data2_1=pd.merge(data2, daytransaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "# 同一用户相同mac1操作的数据信息\n",
    "gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1', \n",
    "                           'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "                           'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "                           'ip1_sub']].groupby([\"mac1\"],as_index=False)\n",
    "\n",
    "mac1transaction_new_count = gby.count()\n",
    "mac1transaction_new_count.columns = ['UID','mac1_channel_count', 'mac1_day_count',  'mac1_trans_amt_count', 'mac1_amt_src1_count', 'mac1_merchant_count',\n",
    "                                    'mac1_code1_count','mac1_code2_count', 'mac1_trans_type1_count', 'mac1_acc_id1_count','mac1_device_code1_count',\n",
    "                                    'mac1_device_code2_count','mac1_device_code3_count', 'mac1_device1_count', 'mac1_device2_count', \n",
    "                                    'mac1_mac1_count','mac1_ip1_count','mac1_bal_count', 'mac1_amt_src2_count', 'mac1_acc_id2_count', \n",
    "                                    'mac1_acc_id3_count', 'mac1_geo_code_count', 'mac1_trans_type2_count', 'mac1_market_code_count',\n",
    "                                    'mac1_market_type_count', 'mac1_ip1_sub_count']\n",
    "data2_2=pd.merge(data2_1, mac1transaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "# # 同一用户在同一天内操作的数据信息\n",
    "# gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1', \n",
    "#                            'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "#                            'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "#                            'ip1_sub']].groupby([\"ip1\"],as_index=False)\n",
    "\n",
    "# ip1transaction_new_count = gby.count()\n",
    "# ip1transaction_new_count.columns = ['UID','ip1_channel_count', 'ip1_day_count',  'ip1_trans_amt_count', 'ip1_amt_src1_count', 'ip1_merchant_count',\n",
    "#                                     'ip1_code1_count','ip1_code2_count', 'ip1_trans_type1_count', 'ip1_acc_id1_count','ip1_device_code1_count',\n",
    "#                                     'ip1_device_code2_count','ip1_device_code3_count', 'ip1_device1_count', 'ip1_device2_count', \n",
    "#                                     'ip1_mac1_count','ip1_ip1_count','ip1_bal_count', 'ip1_amt_src2_count', 'ip1_acc_id2_count', \n",
    "#                                     'ip1_acc_id3_count', 'ip1_geo_code_count', 'ip1_trans_type2_count', 'ip1_market_code_count',\n",
    "#                                     'ip1_market_type_count', 'ip1_ip1_sub_count']\n",
    "# data2_3=pd.merge(data2_2, ip1transaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "\n",
    "# # 同一用户在同一天内操作的数据信息\n",
    "# gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1', \n",
    "#                            'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "#                            'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "#                            'ip1_sub']].groupby([\"acc_id1\"],as_index=False)\n",
    "\n",
    "# acc_id1transaction_new_count = gby.count()\n",
    "# acc_id1transaction_new_count.columns = ['UID','acc_id1_channel_count', 'acc_id1_day_count',  'acc_id1_trans_amt_count', 'acc_id1_amt_src1_count', 'acc_id1_merchant_count',\n",
    "#                                     'acc_id1_code1_count','acc_id1_code2_count', 'acc_id1_trans_type1_count', 'acc_id1_acc_id1_count','acc_id1_device_code1_count',\n",
    "#                                     'acc_id1_device_code2_count','acc_id1_device_code3_count', 'acc_id1_device1_count', 'acc_id1_device2_count', \n",
    "#                                     'acc_id1_mac1_count','acc_id1_ip1_count','acc_id1_bal_count', 'acc_id1_amt_src2_count', 'acc_id1_acc_id2_count', \n",
    "#                                     'acc_id1_acc_id3_count', 'acc_id1_geo_code_count', 'acc_id1_trans_type2_count', 'acc_id1_market_code_count',\n",
    "#                                     'acc_id1_market_type_count', 'acc_id1_ip1_sub_count']\n",
    "# data2_4=pd.merge(data2_3, acc_id1transaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "# # 同一用户在同一天内操作的数据信息\n",
    "# gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1',\n",
    "#                            'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "#                            'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "#                            'ip1_sub']].groupby([\"acc_id2\"],as_index=False)\n",
    "\n",
    "# acc_id2transaction_new_count = gby.count()\n",
    "# acc_id2transaction_new_count.columns = ['UID','acc_id2_channel_count', 'acc_id2_day_count',  'acc_id2_trans_amt_count', 'acc_id2_amt_src1_count', 'acc_id2_merchant_count',\n",
    "#                                     'acc_id2_code1_count','acc_id2_code2_count', 'acc_id2_trans_type1_count', 'acc_id2_acc_id1_count','acc_id2_device_code1_count',\n",
    "#                                     'acc_id2_device_code2_count','acc_id2_device_code3_count', 'acc_id2_device1_count', 'acc_id2_device2_count',\n",
    "#                                     'acc_id2_mac1_count','acc_id2_ip1_count','acc_id2_bal_count', 'acc_id2_amt_src2_count', 'acc_id2_acc_id2_count',\n",
    "#                                     'acc_id2_acc_id3_count', 'acc_id2_geo_code_count', 'acc_id2_trans_type2_count', 'acc_id2_market_code_count',\n",
    "#                                     'acc_id2_market_type_count', 'acc_id2_ip1_sub_count']\n",
    "# data2_5=pd.merge(data2_4, acc_id2transaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "# # 同一用户在同一天内操作的数据信息\n",
    "# gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1',\n",
    "#                            'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "#                            'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "#                            'ip1_sub']].groupby([\"acc_id3\"],as_index=False)\n",
    "\n",
    "# acc_id3transaction_new_count = gby.count()\n",
    "# acc_id3transaction_new_count.columns = ['UID','acc_id3_channel_count', 'acc_id3_day_count',  'acc_id3_trans_amt_count', 'acc_id3_amt_src1_count', 'acc_id3_merchant_count',\n",
    "#                                     'acc_id3_code1_count','acc_id3_code2_count', 'acc_id3_trans_type1_count', 'acc_id3_acc_id1_count','acc_id3_device_code1_count',\n",
    "#                                     'acc_id3_device_code2_count','acc_id3_device_code3_count', 'acc_id3_device1_count', 'acc_id3_device2_count',\n",
    "#                                     'acc_id3_mac1_count','acc_id3_ip1_count','acc_id3_bal_count', 'acc_id3_amt_src2_count', 'acc_id3_acc_id2_count',\n",
    "#                                     'acc_id3_acc_id3_count', 'acc_id3_geo_code_count', 'acc_id3_trans_type2_count', 'acc_id3_market_code_count',\n",
    "#                                     'acc_id3_market_type_count', 'acc_id3_ip1_sub_count']\n",
    "# data2_6=pd.merge(data2_5, acc_id3transaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "# # 同一用户在同一天内操作的数据信息\n",
    "# gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1',\n",
    "#                            'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "#                            'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "#                            'ip1_sub']].groupby([\"geo_code\"],as_index=False)\n",
    "\n",
    "# geo_codetransaction_new_count = gby.count()\n",
    "# geo_codetransaction_new_count.columns = ['UID','geo_code_channel_count', 'geo_code_day_count',  'geo_code_trans_amt_count', 'geo_code_amt_src1_count', 'geo_code_merchant_count',\n",
    "#                                     'geo_code_code1_count','geo_code_code2_count', 'geo_code_trans_type1_count', 'geo_code_acc_id1_count','geo_code_device_code1_count',\n",
    "#                                     'geo_code_device_code2_count','geo_code_device_code3_count', 'geo_code_device1_count', 'geo_code_device2_count',\n",
    "#                                     'geo_code_mac1_count','geo_code_ip1_count','geo_code_bal_count', 'geo_code_amt_src2_count', 'geo_code_acc_id2_count',\n",
    "#                                     'geo_code_acc_id3_count', 'geo_code_geo_code_count', 'geo_code_trans_type2_count', 'geo_code_market_code_count',\n",
    "#                                     'geo_code_market_type_count', 'geo_code_ip1_sub_count']\n",
    "# data2_7=pd.merge(data2_6, geo_codetransaction_new_count, how='left', on = \"UID\")\n",
    "\n",
    "# # 同一用户在同一天内操作的数据信息\n",
    "# gby=transaction_new.loc[:,['UID', 'channel', 'day', 'trans_amt', 'amt_src1', 'merchant', 'code1', 'code2', 'trans_type1',\n",
    "#                            'acc_id1', 'device_code1', 'device_code2', 'device_code3', 'device1', 'device2', 'mac1', 'ip1', \n",
    "#                            'bal', 'amt_src2', 'acc_id2', 'acc_id3', 'geo_code', 'trans_type2', 'market_code', 'market_type', \n",
    "#                            'ip1_sub']].groupby([\"ip1_sub\"],as_index=False)\n",
    "\n",
    "# ip1_subtransaction_new_count = gby.count()\n",
    "# ip1_subtransaction_new_count.columns = ['UID','ip1_sub_channel_count', 'ip1_sub_day_count',  'ip1_sub_trans_amt_count', 'ip1_sub_amt_src1_count', 'ip1_sub_merchant_count',\n",
    "#                                     'ip1_sub_code1_count','ip1_sub_code2_count', 'ip1_sub_trans_type1_count', 'ip1_sub_acc_id1_count','ip1_sub_device_code1_count',\n",
    "#                                     'ip1_sub_device_code2_count','ip1_sub_device_code3_count', 'ip1_sub_device1_count', 'ip1_sub_device2_count',\n",
    "#                                     'ip1_sub_mac1_count','ip1_sub_ip1_count','ip1_sub_bal_count', 'ip1_sub_amt_src2_count', 'ip1_sub_acc_id2_count',\n",
    "#                                     'ip1_sub_acc_id3_count', 'ip1_sub_geo_code_count', 'ip1_sub_trans_type2_count', 'ip1_sub_market_code_count',\n",
    "#                                     'ip1_sub_market_type_count', 'ip1_sub_ip1_sub_count']\n",
    "# data2_8=pd.merge(data2_7, ip1_subtransaction_new_count, how='left', on = \"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_train_new = pd.read_csv(\"../data/tag_train_new.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 给test制作标签，加以区分train和test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_round1_new = operation_round1_new.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "transaction_round1_new = transaction_round1_new.drop_duplicates([\"UID\"], keep=\"last\")\n",
    "test_C = pd.merge(operation_round1_new, transaction_round1_new, on=\"UID\", how=\"outer\")\n",
    "prob_sample = pd.DataFrame({\"UID\": test_C.UID, \"Tag\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data1_3, data2_2, how=\"outer\", on=\"UID\").drop_duplicates(\"UID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得train数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(data, tag_train_new, how=\"inner\", on=\"UID\")\n",
    "train = train.sort_values(\"UID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(data, prob_sample, how=\"inner\", on=\"UID\")\n",
    "test = test.sort_values(\"UID\")\n",
    "test = test.drop([\"Tag\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"./data_feat/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"./data_feat/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "params = {\"booster\": \"gbtree\",\n",
    "          \"objective\" : \"binary:logistic\",\n",
    "          \"eval_metric\" : \"auc\",\n",
    "          \"verbose\" : 1, \n",
    "          \"eta\" : 0.01, \n",
    "          \"max_delta_step\" : 5, \n",
    "          \"max_depth\" : 20, \n",
    "          \"alpha\" : 2, \n",
    "          \"lambda\" : 2, \n",
    "          \"gamma\" : 0.5, \n",
    "          \"subsample\" : 0.8, \n",
    "          \"colsample_bytree\" : 0.8, \n",
    "#           \"colsample_bylevel\" : 0.8, \n",
    "          \"min_child_weight\" : 5, \n",
    "          \"scale_pos_weight\" : 0.8,\n",
    "          \"n_jobs\" : -1}\n",
    "\n",
    "train_Y = train.Tag\n",
    "train_X = train.drop([ \"Tag\"], axis=1)\n",
    "\n",
    "x_train, x_val, y_train, y_true = train_test_split(train_X, train_Y, test_size=0.2, random_state=2018)\n",
    "\n",
    "matrix_train = xgb.DMatrix(x_train, y_train)\n",
    "matrix_test = xgb.DMatrix(x_val)\n",
    "a_model = xgb.train(params, matrix_train, num_boost_round=500)\n",
    "y_predict = a_model.predict(matrix_test)\n",
    "\n",
    "\n",
    "d = pd.DataFrame()\n",
    "d['prob'] = list(y_predict)\n",
    "d['y'] = list(y_true)\n",
    "d = d.sort_values(['prob'], ascending=[0])\n",
    "y = d.y\n",
    "PosAll = pd.Series(y).value_counts()[1]\n",
    "NegAll = pd.Series(y).value_counts()[0]\n",
    "pCumsum = d['y'].cumsum()\n",
    "nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "pCumsumPer = pCumsum / PosAll\n",
    "nCumsumPer = nCumsum / NegAll\n",
    "TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "\n",
    "result = 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "print(\"*******************************************************************************************************\")\n",
    "print(\"result:\", result)\n",
    "print(\"*******************************************************************************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "params = {\"booster\": \"gbtree\",\n",
    "          \"objective\" : \"binary:logistic\",\n",
    "          \"eval_metric\" : \"auc\",\n",
    "          \"verbose\" : 1, \n",
    "          \"eta\" : 0.01, \n",
    "          \"max_delta_step\" : 5, \n",
    "          \"max_depth\" : 20, \n",
    "          \"alpha\" : 2, \n",
    "          \"lambda\" : 2, \n",
    "          \"gamma\" : 0.5, \n",
    "          \"subsample\" : 0.8, \n",
    "          \"colsample_bytree\" : 0.8, \n",
    "#           \"colsample_bylevel\" : 0.8, \n",
    "          \"min_child_weight\" : 5, \n",
    "          \"scale_pos_weight\" : 0.8,\n",
    "          \"n_jobs\" : 6}\n",
    "\n",
    "train_Y = train.Tag\n",
    "train_X = train.drop([ \"Tag\"], axis=1)\n",
    "test_UID = test.UID\n",
    "test_X = test\n",
    "train_matrix = xgb.DMatrix(train_X, train_Y)\n",
    "test_matrix = xgb.DMatrix(test_X)\n",
    "\n",
    "model = xgb.train(params, train_matrix, num_boost_round=500)\n",
    "predict = model.predict(test_matrix)\n",
    "\n",
    "result = pd.DataFrame({\"UID\": test_UID, \"Tag\": predict})\n",
    "result.to_csv(\"../submission/\"+\"result_time\"+str(int(time.strftime(\"%Y%m%d%H%M%S\", time.localtime(time.time()))))+\".csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "feature_score = model.get_fscore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_score = sorted(feature_score.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pd.DataFrame(feature_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
