{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_version(data,label,name):\n",
    "    data['date']=pd.to_datetime(('2018/10/'+data['day'].astype(str)+' '+data['time'].astype(str)),format='%Y/%m/%d %H:%M:%S')\n",
    "    data=data.set_index('date')\n",
    "    data=data.sort_index()\n",
    "    data['device_code']=(data['device_code3'].fillna('')+data['device_code2'].fillna('')+data['device_code1'].fillna(''))  \n",
    "    dave=data[data['version'].notnull()]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    class_le = LabelEncoder()\n",
    "    dave['version_c'] = class_le.fit_transform(dave['version'].values)\n",
    "    dave['ver_dev_max']=dave['version_c'].groupby(dave['UID']).transform(lambda x:x.rolling(window = 2,min_periods = 1).max())\n",
    "    dave['ver_dev_min']=dave['version_c'].groupby(dave['UID']).transform(lambda x:x.rolling(window = 2,min_periods = 1).min())\n",
    "    dave['ver_dev_nui']=0\n",
    "    dave['ver_dev_nui'][dave['ver_dev_max']!=dave['ver_dev_min']]=1\n",
    "#    print(dave[['ver_dev_max','ver_dev_min','ver_dev_nui']])\n",
    "    \n",
    "    dave['ver_dev_nui_1d_sum']=dave.groupby(by='UID')['ver_dev_nui'].transform(lambda x:x.rolling(window='1d').sum())\n",
    "    dave['ver_dev_nui_3d_sum']=dave.groupby(by='UID')['ver_dev_nui'].transform(lambda x:x.rolling(window='3d').sum())\n",
    "    dave['ver_dev_nui_1h_sum']=dave.groupby(by='UID')['ver_dev_nui'].transform(lambda x:x.rolling(window='3h').sum())\n",
    "    \n",
    "    data['ver_dev_nui_1d_sum']=np.nan\n",
    "    data['ver_dev_nui_3d_sum']=np.nan\n",
    "    data['ver_dev_nui_1h_sum']=np.nan\n",
    "    \n",
    "    data['ver_dev_nui_1d_sum'][~data['version'].isnull()]=dave['ver_dev_nui_1d_sum']\n",
    "    data['ver_dev_nui_3d_sum'][~data['version'].isnull()]=dave['ver_dev_nui_3d_sum']\n",
    "    data['ver_dev_nui_1h_sum'][~data['version'].isnull()]=dave['ver_dev_nui_1h_sum']\n",
    "    \n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1d_sum'].sum().reset_index(),on='UID',how='left',suffixes=('_sum', '_max'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1d_sum'].max().reset_index(),on='UID',how='left',suffixes=('', '_max'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1d_sum'].min().reset_index(),on='UID',how='left',suffixes=('', '_min'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1d_sum'].mean().reset_index(),on='UID',how='left',suffixes=('', '_mean'))\n",
    "    \n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_3d_sum'].sum().reset_index(),on='UID',how='left',suffixes=('_sum', '_max'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_3d_sum'].max().reset_index(),on='UID',how='left',suffixes=('', '_max'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_3d_sum'].min().reset_index(),on='UID',how='left',suffixes=('', '_min'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_3d_sum'].mean().reset_index(),on='UID',how='left',suffixes=('', '_mean'))\n",
    "    \n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1h_sum'].sum().reset_index(),on='UID',how='left',suffixes=('_sum', '_max'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1h_sum'].max().reset_index(),on='UID',how='left',suffixes=('', '_max'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1h_sum'].min().reset_index(),on='UID',how='left',suffixes=('', '_min'))\n",
    "    label =label.merge(data.groupby(['UID'])['ver_dev_nui_1h_sum'].mean().reset_index(),on='UID',how='left',suffixes=('', '_mean'))\n",
    "    return lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day\n",
      "day\n",
      "trans_amt\n",
      "bal\n",
      "Done\n",
      "day\n",
      "day\n",
      "trans_amt\n",
      "bal\n",
      "Done\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.2388\tvalid_1's binary_logloss: 0.245047\n",
      "[100]\tvalid_0's binary_logloss: 0.173022\tvalid_1's binary_logloss: 0.184074\n",
      "[150]\tvalid_0's binary_logloss: 0.136071\tvalid_1's binary_logloss: 0.151642\n",
      "[200]\tvalid_0's binary_logloss: 0.112872\tvalid_1's binary_logloss: 0.132376\n",
      "[250]\tvalid_0's binary_logloss: 0.0971642\tvalid_1's binary_logloss: 0.120281\n",
      "[300]\tvalid_0's binary_logloss: 0.0859908\tvalid_1's binary_logloss: 0.112664\n",
      "[350]\tvalid_0's binary_logloss: 0.0775841\tvalid_1's binary_logloss: 0.107641\n",
      "[400]\tvalid_0's binary_logloss: 0.0709517\tvalid_1's binary_logloss: 0.103971\n",
      "[450]\tvalid_0's binary_logloss: 0.0655413\tvalid_1's binary_logloss: 0.101287\n",
      "[500]\tvalid_0's binary_logloss: 0.060903\tvalid_1's binary_logloss: 0.0990213\n",
      "[550]\tvalid_0's binary_logloss: 0.0568439\tvalid_1's binary_logloss: 0.0971606\n",
      "[600]\tvalid_0's binary_logloss: 0.0534213\tvalid_1's binary_logloss: 0.0957027\n",
      "[650]\tvalid_0's binary_logloss: 0.0503986\tvalid_1's binary_logloss: 0.0944639\n",
      "[700]\tvalid_0's binary_logloss: 0.0477629\tvalid_1's binary_logloss: 0.0935687\n",
      "[750]\tvalid_0's binary_logloss: 0.0454251\tvalid_1's binary_logloss: 0.0929646\n",
      "[800]\tvalid_0's binary_logloss: 0.0432916\tvalid_1's binary_logloss: 0.0924328\n",
      "[850]\tvalid_0's binary_logloss: 0.0413515\tvalid_1's binary_logloss: 0.0919786\n",
      "[900]\tvalid_0's binary_logloss: 0.0395348\tvalid_1's binary_logloss: 0.0916174\n",
      "[950]\tvalid_0's binary_logloss: 0.0379735\tvalid_1's binary_logloss: 0.09136\n",
      "[1000]\tvalid_0's binary_logloss: 0.0365141\tvalid_1's binary_logloss: 0.0911288\n",
      "[1050]\tvalid_0's binary_logloss: 0.0351763\tvalid_1's binary_logloss: 0.0908652\n",
      "[1100]\tvalid_0's binary_logloss: 0.033952\tvalid_1's binary_logloss: 0.0907022\n",
      "[1150]\tvalid_0's binary_logloss: 0.0327933\tvalid_1's binary_logloss: 0.0906358\n",
      "[1200]\tvalid_0's binary_logloss: 0.0317303\tvalid_1's binary_logloss: 0.0905962\n",
      "Early stopping, best iteration is:\n",
      "[1189]\tvalid_0's binary_logloss: 0.0319597\tvalid_1's binary_logloss: 0.0905793\n",
      "[0.09057927843252615]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.238801\tvalid_1's binary_logloss: 0.244589\n",
      "[100]\tvalid_0's binary_logloss: 0.172756\tvalid_1's binary_logloss: 0.183201\n",
      "[150]\tvalid_0's binary_logloss: 0.13571\tvalid_1's binary_logloss: 0.150392\n",
      "[200]\tvalid_0's binary_logloss: 0.1123\tvalid_1's binary_logloss: 0.130588\n",
      "[250]\tvalid_0's binary_logloss: 0.0966913\tvalid_1's binary_logloss: 0.11827\n",
      "[300]\tvalid_0's binary_logloss: 0.0855957\tvalid_1's binary_logloss: 0.110352\n",
      "[350]\tvalid_0's binary_logloss: 0.0773063\tvalid_1's binary_logloss: 0.105136\n",
      "[400]\tvalid_0's binary_logloss: 0.0707647\tvalid_1's binary_logloss: 0.101478\n",
      "[450]\tvalid_0's binary_logloss: 0.0653803\tvalid_1's binary_logloss: 0.098714\n",
      "[500]\tvalid_0's binary_logloss: 0.0607862\tvalid_1's binary_logloss: 0.0965578\n",
      "[550]\tvalid_0's binary_logloss: 0.0568562\tvalid_1's binary_logloss: 0.0947425\n",
      "[600]\tvalid_0's binary_logloss: 0.0534422\tvalid_1's binary_logloss: 0.0933614\n",
      "[650]\tvalid_0's binary_logloss: 0.0504202\tvalid_1's binary_logloss: 0.0922333\n",
      "[700]\tvalid_0's binary_logloss: 0.0478222\tvalid_1's binary_logloss: 0.0914698\n",
      "[750]\tvalid_0's binary_logloss: 0.0454298\tvalid_1's binary_logloss: 0.09082\n",
      "[800]\tvalid_0's binary_logloss: 0.0432953\tvalid_1's binary_logloss: 0.0903043\n",
      "[850]\tvalid_0's binary_logloss: 0.0413723\tvalid_1's binary_logloss: 0.0898142\n",
      "[900]\tvalid_0's binary_logloss: 0.0396191\tvalid_1's binary_logloss: 0.08938\n",
      "[950]\tvalid_0's binary_logloss: 0.0380484\tvalid_1's binary_logloss: 0.0890949\n",
      "[1000]\tvalid_0's binary_logloss: 0.0366129\tvalid_1's binary_logloss: 0.0888955\n",
      "[1050]\tvalid_0's binary_logloss: 0.0352846\tvalid_1's binary_logloss: 0.0886941\n",
      "[1100]\tvalid_0's binary_logloss: 0.0340464\tvalid_1's binary_logloss: 0.0885919\n",
      "[1150]\tvalid_0's binary_logloss: 0.0329055\tvalid_1's binary_logloss: 0.0885117\n",
      "[1200]\tvalid_0's binary_logloss: 0.0318485\tvalid_1's binary_logloss: 0.0883925\n",
      "[1250]\tvalid_0's binary_logloss: 0.0308627\tvalid_1's binary_logloss: 0.0883714\n",
      "Early stopping, best iteration is:\n",
      "[1228]\tvalid_0's binary_logloss: 0.0312878\tvalid_1's binary_logloss: 0.0883364\n",
      "[0.09057927843252615, 0.08833641956789949]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.238099\tvalid_1's binary_logloss: 0.246603\n",
      "[100]\tvalid_0's binary_logloss: 0.172131\tvalid_1's binary_logloss: 0.186461\n",
      "[150]\tvalid_0's binary_logloss: 0.134765\tvalid_1's binary_logloss: 0.154253\n",
      "[200]\tvalid_0's binary_logloss: 0.111313\tvalid_1's binary_logloss: 0.135519\n",
      "[250]\tvalid_0's binary_logloss: 0.0956293\tvalid_1's binary_logloss: 0.124212\n",
      "[300]\tvalid_0's binary_logloss: 0.0844557\tvalid_1's binary_logloss: 0.117021\n",
      "[350]\tvalid_0's binary_logloss: 0.0759936\tvalid_1's binary_logloss: 0.112148\n",
      "[400]\tvalid_0's binary_logloss: 0.0693693\tvalid_1's binary_logloss: 0.108931\n",
      "[450]\tvalid_0's binary_logloss: 0.0639535\tvalid_1's binary_logloss: 0.106644\n",
      "[500]\tvalid_0's binary_logloss: 0.0594106\tvalid_1's binary_logloss: 0.104854\n",
      "[550]\tvalid_0's binary_logloss: 0.0554489\tvalid_1's binary_logloss: 0.103403\n",
      "[600]\tvalid_0's binary_logloss: 0.0520622\tvalid_1's binary_logloss: 0.102322\n",
      "[650]\tvalid_0's binary_logloss: 0.0490885\tvalid_1's binary_logloss: 0.101438\n",
      "[700]\tvalid_0's binary_logloss: 0.0464227\tvalid_1's binary_logloss: 0.100739\n",
      "[750]\tvalid_0's binary_logloss: 0.0441154\tvalid_1's binary_logloss: 0.100235\n",
      "[800]\tvalid_0's binary_logloss: 0.0420946\tvalid_1's binary_logloss: 0.0998194\n",
      "[850]\tvalid_0's binary_logloss: 0.0402262\tvalid_1's binary_logloss: 0.0994926\n",
      "[900]\tvalid_0's binary_logloss: 0.0385282\tvalid_1's binary_logloss: 0.0991698\n",
      "[950]\tvalid_0's binary_logloss: 0.0369461\tvalid_1's binary_logloss: 0.0989831\n",
      "[1000]\tvalid_0's binary_logloss: 0.0355407\tvalid_1's binary_logloss: 0.0988438\n",
      "[1050]\tvalid_0's binary_logloss: 0.0342247\tvalid_1's binary_logloss: 0.0986898\n",
      "[1100]\tvalid_0's binary_logloss: 0.0330499\tvalid_1's binary_logloss: 0.0986177\n",
      "[1150]\tvalid_0's binary_logloss: 0.0319484\tvalid_1's binary_logloss: 0.0985618\n",
      "Early stopping, best iteration is:\n",
      "[1138]\tvalid_0's binary_logloss: 0.0321989\tvalid_1's binary_logloss: 0.0985528\n",
      "[0.09057927843252615, 0.08833641956789949, 0.09855279911854632]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.238148\tvalid_1's binary_logloss: 0.246676\n",
      "[100]\tvalid_0's binary_logloss: 0.172056\tvalid_1's binary_logloss: 0.186582\n",
      "[150]\tvalid_0's binary_logloss: 0.134985\tvalid_1's binary_logloss: 0.154092\n",
      "[200]\tvalid_0's binary_logloss: 0.111836\tvalid_1's binary_logloss: 0.134949\n",
      "[250]\tvalid_0's binary_logloss: 0.0960743\tvalid_1's binary_logloss: 0.12305\n",
      "[300]\tvalid_0's binary_logloss: 0.0850236\tvalid_1's binary_logloss: 0.115397\n",
      "[350]\tvalid_0's binary_logloss: 0.0767184\tvalid_1's binary_logloss: 0.110181\n",
      "[400]\tvalid_0's binary_logloss: 0.0701712\tvalid_1's binary_logloss: 0.106338\n",
      "[450]\tvalid_0's binary_logloss: 0.0648109\tvalid_1's binary_logloss: 0.103425\n",
      "[500]\tvalid_0's binary_logloss: 0.0602533\tvalid_1's binary_logloss: 0.101233\n",
      "[550]\tvalid_0's binary_logloss: 0.0562606\tvalid_1's binary_logloss: 0.0994179\n",
      "[600]\tvalid_0's binary_logloss: 0.0527884\tvalid_1's binary_logloss: 0.098011\n",
      "[650]\tvalid_0's binary_logloss: 0.0498419\tvalid_1's binary_logloss: 0.0969016\n",
      "[700]\tvalid_0's binary_logloss: 0.0471815\tvalid_1's binary_logloss: 0.0961022\n",
      "[750]\tvalid_0's binary_logloss: 0.0448145\tvalid_1's binary_logloss: 0.0954456\n",
      "[800]\tvalid_0's binary_logloss: 0.0426829\tvalid_1's binary_logloss: 0.0949123\n",
      "[850]\tvalid_0's binary_logloss: 0.0407711\tvalid_1's binary_logloss: 0.09447\n",
      "[900]\tvalid_0's binary_logloss: 0.0390549\tvalid_1's binary_logloss: 0.0941174\n",
      "[950]\tvalid_0's binary_logloss: 0.0374769\tvalid_1's binary_logloss: 0.0938505\n",
      "[1000]\tvalid_0's binary_logloss: 0.0360416\tvalid_1's binary_logloss: 0.0936387\n",
      "[1050]\tvalid_0's binary_logloss: 0.0347134\tvalid_1's binary_logloss: 0.0934896\n",
      "[1100]\tvalid_0's binary_logloss: 0.0335026\tvalid_1's binary_logloss: 0.0933452\n",
      "[1150]\tvalid_0's binary_logloss: 0.0323746\tvalid_1's binary_logloss: 0.0932091\n",
      "[1200]\tvalid_0's binary_logloss: 0.031321\tvalid_1's binary_logloss: 0.0931315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1219]\tvalid_0's binary_logloss: 0.0309343\tvalid_1's binary_logloss: 0.0931033\n",
      "[0.09057927843252615, 0.08833641956789949, 0.09855279911854632, 0.09310327353816128]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.239054\tvalid_1's binary_logloss: 0.243539\n",
      "[100]\tvalid_0's binary_logloss: 0.173371\tvalid_1's binary_logloss: 0.182691\n",
      "[150]\tvalid_0's binary_logloss: 0.136129\tvalid_1's binary_logloss: 0.149643\n",
      "[200]\tvalid_0's binary_logloss: 0.112607\tvalid_1's binary_logloss: 0.129987\n",
      "[250]\tvalid_0's binary_logloss: 0.0968216\tvalid_1's binary_logloss: 0.11793\n",
      "[300]\tvalid_0's binary_logloss: 0.0856648\tvalid_1's binary_logloss: 0.110298\n",
      "[350]\tvalid_0's binary_logloss: 0.0771802\tvalid_1's binary_logloss: 0.105116\n",
      "[400]\tvalid_0's binary_logloss: 0.0705658\tvalid_1's binary_logloss: 0.10145\n",
      "[450]\tvalid_0's binary_logloss: 0.065187\tvalid_1's binary_logloss: 0.0991397\n",
      "[500]\tvalid_0's binary_logloss: 0.0606103\tvalid_1's binary_logloss: 0.0973283\n",
      "[550]\tvalid_0's binary_logloss: 0.0566947\tvalid_1's binary_logloss: 0.0960652\n",
      "[600]\tvalid_0's binary_logloss: 0.0531977\tvalid_1's binary_logloss: 0.0948585\n",
      "[650]\tvalid_0's binary_logloss: 0.050134\tvalid_1's binary_logloss: 0.0939052\n",
      "[700]\tvalid_0's binary_logloss: 0.0475131\tvalid_1's binary_logloss: 0.0932876\n",
      "[750]\tvalid_0's binary_logloss: 0.0451304\tvalid_1's binary_logloss: 0.092675\n",
      "[800]\tvalid_0's binary_logloss: 0.0430079\tvalid_1's binary_logloss: 0.0922009\n",
      "[850]\tvalid_0's binary_logloss: 0.0410767\tvalid_1's binary_logloss: 0.0917774\n",
      "[900]\tvalid_0's binary_logloss: 0.0393173\tvalid_1's binary_logloss: 0.0914587\n",
      "[950]\tvalid_0's binary_logloss: 0.0377101\tvalid_1's binary_logloss: 0.0912855\n",
      "[1000]\tvalid_0's binary_logloss: 0.0362799\tvalid_1's binary_logloss: 0.0910335\n",
      "[1050]\tvalid_0's binary_logloss: 0.0349516\tvalid_1's binary_logloss: 0.0908698\n",
      "[1100]\tvalid_0's binary_logloss: 0.033732\tvalid_1's binary_logloss: 0.0907945\n",
      "[1150]\tvalid_0's binary_logloss: 0.0326115\tvalid_1's binary_logloss: 0.090796\n",
      "Early stopping, best iteration is:\n",
      "[1137]\tvalid_0's binary_logloss: 0.0328859\tvalid_1's binary_logloss: 0.0907686\n",
      "[0.09057927843252615, 0.08833641956789949, 0.09855279911854632, 0.09310327353816128, 0.09076861370695899]\n",
      "0.7991365227537923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "D:\\python3\\lib\\site-packages\\ipykernel_launcher.py:265: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 'TC_AUC',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True\n",
    "\n",
    "op_train = pd.read_csv('../data/operation_train_new.csv')\n",
    "trans_train = pd.read_csv('../data/transaction_train_new.csv')\n",
    "\n",
    "op_test = pd.read_csv('../data/operation_round1_new.csv')\n",
    "trans_test = pd.read_csv('../data/transaction_round1_new.csv')\n",
    "y = pd.read_csv('../data/tag_train_new.csv')\n",
    "sub = pd.read_csv('../data/sub.csv')\n",
    "\n",
    "\n",
    "def get_feature(op,trans,label):\n",
    "    for feature in op.columns[:]:\n",
    "        if feature not in ['day']:\n",
    "            if feature != 'UID':\n",
    "                label = label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "                label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['ip1','mac1','mac2','geo_code']:\n",
    "                if feature not in deliver:\n",
    "                    if feature != 'UID':\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                    else:\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['ip1','mac1','mac2']:\n",
    "                if feature not in deliver:\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].max().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].min().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].sum().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].mean().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].std().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    for feature in trans.columns[1:]:\n",
    "        if feature not in ['trans_amt','bal','day']:\n",
    "            if feature != 'UID':\n",
    "                label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "                label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['merchant','ip1','mac1','geo_code',]:\n",
    "                if feature not in deliver: \n",
    "                    if feature != 'UID':\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                    else:\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "            #if feature in ['merchant','code2','acc_id1','market_code','market_code']:\n",
    "            #    label[feature+'_z'] = 0 \n",
    "            #    label[feature+'_z'] = label[feature+'_y']/label[feature+'_x']\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['merchant','ip1','mac1']:\n",
    "                if feature not in deliver:\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].max().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].min().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].sum().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].mean().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].std().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    print(\"Done\")\n",
    "    return label\n",
    "\n",
    "\n",
    "train = get_feature(op_train,trans_train,y).fillna(-1)\n",
    "test = get_feature(op_test,trans_test,sub).fillna(-1)\n",
    "\n",
    "train = train.drop(['Tag'],axis = 1).fillna(-1)\n",
    "label = y['Tag']\n",
    "\n",
    "test_id = test['UID']\n",
    "test = test.drop(['Tag'],axis = 1).fillna(-1)\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=100, reg_alpha=3, reg_lambda=5, max_depth=-1,\n",
    "    n_estimators=10000, objective='binary', subsample=0.9, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.01,\n",
    "    random_state=1000, n_jobs=-1, min_child_weight=4, min_child_samples=5, min_split_gain=0)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "best_score = []\n",
    "\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test_id.shape[0])\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, label)):\n",
    "    lgb_model.fit(train.iloc[train_index], label.iloc[train_index], verbose=50,\n",
    "                  eval_set=[(train.iloc[train_index], label.iloc[train_index]),\n",
    "                            (train.iloc[test_index], label.iloc[test_index])], early_stopping_rounds=30)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    oof_preds[test_index] = lgb_model.predict_proba(train.iloc[test_index], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "    test_pred = lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    sub_preds += test_pred / 5\n",
    "    #print('test mean:', test_pred.mean())\n",
    "    #predict_result['predicted_score'] = predict_result['predicted_score'] + test_pred\n",
    "\n",
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])\n",
    "sub = pd.read_csv('../data/sub.csv')\n",
    "sub['Tag'] = sub_preds\n",
    "sub.to_csv('../submission/baseline_%s001.csv'%str(m),index=False)\n",
    "\n",
    "\n",
    "\n",
    "path = '../data/'\n",
    "trans_train = pd.read_csv(path+'transaction_train_new.csv')\n",
    "y = pd.read_csv(path+'tag_train_new.csv')\n",
    "trans_train = trans_train.merge(y,on='UID',how='left')\n",
    "def find_wrong(trans_train,y,feature):\n",
    "    black = (trans_train.groupby([feature])['Tag'].sum()/trans_train.groupby([feature])['Tag'].count()).sort_values(ascending=False)\n",
    "    tag_count = trans_train.groupby([feature])['Tag'].count().reset_index()\n",
    "    black = black.reset_index()\n",
    "    black = black.merge(tag_count,on=feature,how='left')\n",
    "    black = black.sort_values(by = ['Tag_x','Tag_y'],ascending=False)\n",
    "    return black\n",
    "\n",
    "Test_trans = pd.read_csv(path+'transaction_round1_new.csv')\n",
    "Test_tag = pd.read_csv('../submission/baseline_%s001.csv'%str(m)) # 测试样本\n",
    "# rule_code = ['5776870b5747e14e' ,'8b3f74a1391b5427' ,'0e90f47392008def' ,\n",
    "# '6d55ccc689b910ee' ,'2260d61b622795fb' ,'1f72814f76a984fa' ,'c2e87787a76836e0' ,\n",
    "# '4bca6018239c6201' ,'922720f3827ccef8' ,'2b2e7046145d9517' ,'09f911b8dc5dfc32' ,\n",
    "# '7cc961258f4dce9c' ,'bc0213f01c5023ac' ,'0316dca8cc63cc17' ,'c988e79f00cc2dc0' ,\n",
    "# 'd0b1218bae116267' ,'72fac912326004ee' ,'00159b7cc2f1dfc8' ,'49ec5883ba0c1b0e' ,\n",
    "# 'c9c29fc3d44a1d7b' ,'33ce9c3877281764' ,'e7c929127cdefadb' ,'05bc3e22c112c8c9' ,\n",
    "# '5cf4f55246093ccf' ,'6704d8d8d5965303' ,'4df1708c5827264d' ,'6e8b399ffe2d1e80' ,\n",
    "# 'f65104453e0b1d10' ,'1733ddb502eb3923' ,'a086f47f681ad851' ,'1d4372ca8a38cd1f' ,\n",
    "# '29db08e2284ea103' ,'4e286438d39a6bd4' ,'54cb3985d0380ca4' ,'6b64437be7590eb0' ,\n",
    "# '89eb97474a6cb3c6' ,'95d506c0e49a492c' ,'c17b47056178e2bb' ,'d36b25a74285bebb']\n",
    "\n",
    "black = find_wrong(trans_train,y,'merchant')\n",
    "rule_code_1 = black.sort_values(by=['Tag_x','Tag_y'],ascending=False).iloc[:50].merchant.tolist()\n",
    "test_rule_uid = pd.DataFrame(Test_trans[Test_trans['merchant'].isin(rule_code_1)].UID.unique())\n",
    "pred_data_rule = Test_tag.merge(test_rule_uid,left_on ='UID',right_on =0, how ='left')\n",
    "pred_data_rule['Tag'][(pred_data_rule[0]>0)] = 1\n",
    "pred_data_rule[['UID', 'Tag']].to_csv('../submission/sub+rule001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
