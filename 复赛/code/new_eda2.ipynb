{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线上0.7189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day\n",
      "day\n",
      "trans_amt\n",
      "bal\n",
      "Done\n",
      "day\n",
      "day\n",
      "trans_amt\n",
      "bal\n",
      "Done\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0972822\tvalid_1's binary_logloss: 0.1208\n",
      "[100]\tvalid_0's binary_logloss: 0.0612987\tvalid_1's binary_logloss: 0.100403\n",
      "[150]\tvalid_0's binary_logloss: 0.0456604\tvalid_1's binary_logloss: 0.0942558\n",
      "[200]\tvalid_0's binary_logloss: 0.0366983\tvalid_1's binary_logloss: 0.0926076\n",
      "[250]\tvalid_0's binary_logloss: 0.0309615\tvalid_1's binary_logloss: 0.0917793\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.030573\tvalid_1's binary_logloss: 0.0916673\n",
      "[0.09166726514565353]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0968026\tvalid_1's binary_logloss: 0.118532\n",
      "[100]\tvalid_0's binary_logloss: 0.0609438\tvalid_1's binary_logloss: 0.096741\n",
      "[150]\tvalid_0's binary_logloss: 0.0456817\tvalid_1's binary_logloss: 0.0911335\n",
      "[200]\tvalid_0's binary_logloss: 0.036936\tvalid_1's binary_logloss: 0.0896877\n",
      "[250]\tvalid_0's binary_logloss: 0.0309975\tvalid_1's binary_logloss: 0.088922\n",
      "Early stopping, best iteration is:\n",
      "[258]\tvalid_0's binary_logloss: 0.0302693\tvalid_1's binary_logloss: 0.0888195\n",
      "[0.09166726514565353, 0.08881949212821927]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0959692\tvalid_1's binary_logloss: 0.124365\n",
      "[100]\tvalid_0's binary_logloss: 0.0596174\tvalid_1's binary_logloss: 0.105256\n",
      "[150]\tvalid_0's binary_logloss: 0.0442948\tvalid_1's binary_logloss: 0.101298\n",
      "[200]\tvalid_0's binary_logloss: 0.0356642\tvalid_1's binary_logloss: 0.100135\n",
      "[250]\tvalid_0's binary_logloss: 0.0300434\tvalid_1's binary_logloss: 0.0998237\n",
      "[300]\tvalid_0's binary_logloss: 0.0260582\tvalid_1's binary_logloss: 0.0999287\n",
      "Early stopping, best iteration is:\n",
      "[271]\tvalid_0's binary_logloss: 0.0281851\tvalid_1's binary_logloss: 0.0995945\n",
      "[0.09166726514565353, 0.08881949212821927, 0.09959452609162227]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0962254\tvalid_1's binary_logloss: 0.123974\n",
      "[100]\tvalid_0's binary_logloss: 0.0604999\tvalid_1's binary_logloss: 0.102268\n",
      "[150]\tvalid_0's binary_logloss: 0.0452437\tvalid_1's binary_logloss: 0.0969599\n",
      "[200]\tvalid_0's binary_logloss: 0.0363724\tvalid_1's binary_logloss: 0.0951216\n",
      "[250]\tvalid_0's binary_logloss: 0.0305489\tvalid_1's binary_logloss: 0.094579\n",
      "Early stopping, best iteration is:\n",
      "[256]\tvalid_0's binary_logloss: 0.0299705\tvalid_1's binary_logloss: 0.0945248\n",
      "[0.09166726514565353, 0.08881949212821927, 0.09959452609162227, 0.09452479768709851]\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\tvalid_0's binary_logloss: 0.0970427\tvalid_1's binary_logloss: 0.119056\n",
      "[100]\tvalid_0's binary_logloss: 0.0609261\tvalid_1's binary_logloss: 0.0984735\n",
      "[150]\tvalid_0's binary_logloss: 0.0454758\tvalid_1's binary_logloss: 0.09353\n",
      "[200]\tvalid_0's binary_logloss: 0.0367592\tvalid_1's binary_logloss: 0.0924669\n",
      "[250]\tvalid_0's binary_logloss: 0.0309592\tvalid_1's binary_logloss: 0.0918831\n",
      "[300]\tvalid_0's binary_logloss: 0.0268857\tvalid_1's binary_logloss: 0.0918119\n",
      "Early stopping, best iteration is:\n",
      "[288]\tvalid_0's binary_logloss: 0.0277422\tvalid_1's binary_logloss: 0.0917348\n",
      "[0.09166726514565353, 0.08881949212821927, 0.09959452609162227, 0.09452479768709851, 0.09173480864299656]\n",
      "0.7964527421236873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "D:\\python3\\lib\\site-packages\\ipykernel_launcher.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def tpr_weight_funtion(y_true,y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 'TC_AUC',0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3,True\n",
    "\n",
    "op_train = pd.read_csv('../data/operation_train_new.csv')\n",
    "trans_train = pd.read_csv('../data/transaction_train_new.csv')\n",
    "\n",
    "op_test = pd.read_csv('../data/operation_round1_new.csv')\n",
    "trans_test = pd.read_csv('../data/transaction_round1_new.csv')\n",
    "y = pd.read_csv('../data/tag_train_new.csv')\n",
    "sub = pd.read_csv('../data/sub.csv')\n",
    "\n",
    "\n",
    "def get_feature(op,trans,label):\n",
    "    for feature in op.columns[:]:\n",
    "        if feature not in ['day']:\n",
    "            if feature != 'UID':\n",
    "                label = label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "                label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['ip1','mac1','mac2','geo_code']:\n",
    "                if feature not in deliver:\n",
    "                    if feature != 'UID':\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                    else:\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(op.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(op.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['ip1','mac1','mac2']:\n",
    "                if feature not in deliver:\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].max().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].min().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].sum().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].mean().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = op[['UID',deliver]].merge(op.groupby([deliver])[feature].std().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    for feature in trans.columns[1:]:\n",
    "        if feature not in ['trans_amt','bal','day']:\n",
    "            if feature != 'UID':\n",
    "                label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "                label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['merchant','ip1','mac1','geo_code',]:\n",
    "                if feature not in deliver: \n",
    "                    if feature != 'UID':\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                        temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                    else:\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "                        temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID_x','UID_y']] \n",
    "                        temp = temp.groupby('UID_x')['UID_y'].sum().reset_index()\n",
    "                        temp.columns = ['UID',feature+deliver]\n",
    "                        label =label.merge(temp,on='UID',how='left')\n",
    "                        del temp\n",
    "            #if feature in ['merchant','code2','acc_id1','market_code','market_code']:\n",
    "            #    label[feature+'_z'] = 0 \n",
    "            #    label[feature+'_z'] = label[feature+'_y']/label[feature+'_x']\n",
    "        else:\n",
    "            print(feature)\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].count().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].nunique().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].max().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].min().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].sum().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].mean().reset_index(),on='UID',how='left')\n",
    "            label =label.merge(trans.groupby(['UID'])[feature].std().reset_index(),on='UID',how='left')\n",
    "            for deliver in ['merchant','ip1','mac1']:\n",
    "                if feature not in deliver:\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].count().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].nunique().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].sum().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].max().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].min().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].sum().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].mean().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    temp = trans[['UID',deliver]].merge(trans.groupby([deliver])[feature].std().reset_index(),on=deliver,how='left')[['UID',feature]] \n",
    "                    temp = temp.groupby('UID')[feature].mean().reset_index()\n",
    "                    temp.columns = ['UID',feature+deliver]\n",
    "                    label =label.merge(temp,on='UID',how='left')\n",
    "                    del temp\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    print(\"Done\")\n",
    "    return label\n",
    "\n",
    "\n",
    "train = get_feature(op_train,trans_train,y).fillna(-1)\n",
    "test = get_feature(op_test,trans_test,sub).fillna(-1)\n",
    "\n",
    "train = train.drop(['Tag'],axis = 1).fillna(-1)\n",
    "label = y['Tag']\n",
    "\n",
    "test_id = test['UID']\n",
    "test = test.drop(['Tag'],axis = 1).fillna(-1)\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=100, reg_alpha=3, reg_lambda=5, max_depth=-1,\n",
    "    n_estimators=10000, objective='binary', subsample=0.9, colsample_bytree=0.77, subsample_freq=1, learning_rate=0.05,\n",
    "    random_state=1000, n_jobs=-1, min_child_weight=4, min_child_samples=5, min_split_gain=0)\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "best_score = []\n",
    "\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test_id.shape[0])\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train, label)):\n",
    "    lgb_model.fit(train.iloc[train_index], label.iloc[train_index], verbose=50,\n",
    "                  eval_set=[(train.iloc[train_index], label.iloc[train_index]),\n",
    "                            (train.iloc[test_index], label.iloc[test_index])], early_stopping_rounds=30)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    oof_preds[test_index] = lgb_model.predict_proba(train.iloc[test_index], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "    test_pred = lgb_model.predict_proba(test, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    sub_preds += test_pred / 5\n",
    "    #print('test mean:', test_pred.mean())\n",
    "    #predict_result['predicted_score'] = predict_result['predicted_score'] + test_pred\n",
    "\n",
    "m = tpr_weight_funtion(y_predict=oof_preds,y_true=label)\n",
    "print(m[1])\n",
    "sub = pd.read_csv('../data/sub.csv')\n",
    "sub['Tag'] = sub_preds\n",
    "sub.to_csv('../submission/baseline_%s.csv'%str(m),index=False)\n",
    "\n",
    "\n",
    "\n",
    "path = '../data/'\n",
    "trans_train = pd.read_csv(path+'transaction_train_new.csv')\n",
    "y = pd.read_csv(path+'tag_train_new.csv')\n",
    "trans_train = trans_train.merge(y,on='UID',how='left')\n",
    "def find_wrong(trans_train,y,feature):\n",
    "    black = (trans_train.groupby([feature])['Tag'].sum()/trans_train.groupby([feature])['Tag'].count()).sort_values(ascending=False)\n",
    "    tag_count = trans_train.groupby([feature])['Tag'].count().reset_index()\n",
    "    black = black.reset_index()\n",
    "    black = black.merge(tag_count,on=feature,how='left')\n",
    "    black = black.sort_values(by = ['Tag_x','Tag_y'],ascending=False)\n",
    "    return black\n",
    "\n",
    "Test_trans = pd.read_csv(path+'transaction_round1_new.csv')\n",
    "Test_tag = pd.read_csv('../submission/baseline_%s.csv'%str(m)) # 测试样本\n",
    "# rule_code = ['5776870b5747e14e' ,'8b3f74a1391b5427' ,'0e90f47392008def' ,\n",
    "# '6d55ccc689b910ee' ,'2260d61b622795fb' ,'1f72814f76a984fa' ,'c2e87787a76836e0' ,\n",
    "# '4bca6018239c6201' ,'922720f3827ccef8' ,'2b2e7046145d9517' ,'09f911b8dc5dfc32' ,\n",
    "# '7cc961258f4dce9c' ,'bc0213f01c5023ac' ,'0316dca8cc63cc17' ,'c988e79f00cc2dc0' ,\n",
    "# 'd0b1218bae116267' ,'72fac912326004ee' ,'00159b7cc2f1dfc8' ,'49ec5883ba0c1b0e' ,\n",
    "# 'c9c29fc3d44a1d7b' ,'33ce9c3877281764' ,'e7c929127cdefadb' ,'05bc3e22c112c8c9' ,\n",
    "# '5cf4f55246093ccf' ,'6704d8d8d5965303' ,'4df1708c5827264d' ,'6e8b399ffe2d1e80' ,\n",
    "# 'f65104453e0b1d10' ,'1733ddb502eb3923' ,'a086f47f681ad851' ,'1d4372ca8a38cd1f' ,\n",
    "# '29db08e2284ea103' ,'4e286438d39a6bd4' ,'54cb3985d0380ca4' ,'6b64437be7590eb0' ,\n",
    "# '89eb97474a6cb3c6' ,'95d506c0e49a492c' ,'c17b47056178e2bb' ,'d36b25a74285bebb']\n",
    "\n",
    "black = find_wrong(trans_train,y,'merchant')\n",
    "rule_code_1 = black.sort_values(by=['Tag_x','Tag_y'],ascending=False).iloc[:50].merchant.tolist()\n",
    "test_rule_uid = pd.DataFrame(Test_trans[Test_trans['merchant'].isin(rule_code_1)].UID.unique())\n",
    "pred_data_rule = Test_tag.merge(test_rule_uid,left_on ='UID',right_on =0, how ='left')\n",
    "pred_data_rule['Tag'][(pred_data_rule[0]>0)] = 1\n",
    "pred_data_rule[['UID', 'Tag']].to_csv('../submission/sub+rule.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
